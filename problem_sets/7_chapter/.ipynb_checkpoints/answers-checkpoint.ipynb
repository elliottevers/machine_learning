{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alttext](img/p1a.png)\n",
    "![alttext](img/p1b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**define generalized polynomial over first subdomain**\n",
    "\n",
    "Let \n",
    "$$\n",
    "f _ { 1 } ( x ) = a _ { 1 } + b _ { 1 } x + c _ { 1 } x ^ { 2 } + d _ { 1 } x ^ { 3 }\n",
    "$$.\n",
    "\n",
    "**make it equivalent to polynomial under consideration over first subdomain**\n",
    "\n",
    "We can set $f(x) = f_2 (x)$ for $x \\leq \\xi$ by equating their coefficients\n",
    "$$a _ { 1 } = \\beta _ { 0 }, b _ { 1 } = \\beta _ { 1 }, c _ { 1 } = \\beta _ { 3 }, d _ { 1 } = \\beta _ { 4 }$$.\n",
    "\n",
    "**polynomial under consideration is a generalized polynomial**\n",
    "\n",
    "But $f_1$ is a generalized polynomial for $x \\leq \\xi$, so $f$ is as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**restate our polynomial under consideration**\n",
    "\n",
    "Recall our function\n",
    "$$\n",
    "f ( x ) = \\beta _ { 0 } + \\beta _ { 1 } x + \\beta _ { 2 } x ^ { 2 } + \\beta _ { 3 } x ^ { 3 } + \\beta _ { 4 } ( x - \\xi ) ^ { 3 }\n",
    "$$.\n",
    "\n",
    "**simplify**\n",
    "\n",
    "We can rearrange to produce\n",
    "$$\n",
    "f(x) = \\left( \\beta _ { 0 } - \\beta _ { 4 } \\xi ^ { 3 } \\right) + \\left( \\beta _ { 1 } + 3 \\beta _ { 4 } \\xi ^ { 2 } \\right) x + \\beta _ { 3 } x ^ { 3 } + \\beta _ { 4 } \\left( x ^ { 3 } - 3 \\xi x ^ { 2 } + 3 \\xi ^ { 2 } x - \\xi ^ { 3 } \\right)\n",
    "$$.\n",
    "\n",
    "**define generalized polynomial over second subdomain**\n",
    "\n",
    "Let \n",
    "$$\n",
    "f _ { 2 } ( x ) = a _ { 2 } + b _ { 2 } x + c _ { 2 } x ^ { 2 } + d _ { 2 } x ^ { 3 }\n",
    "$$ be a generalized polynomial.\n",
    "\n",
    "\n",
    "**make polynomials equivalent**\n",
    "\n",
    "We can set $f(x) = f_2 (x)$ by equating their coefficients\n",
    "$$\n",
    "a _ { 2 } = \\beta _ { 0 } - \\beta _ { 4 } \\xi ^ { 3 }, \\\\\n",
    "b _ { 2 } = \\beta _ { 1 } + 3 \\beta _ { 4 } \\xi ^ { 2 }, \\\\\n",
    "c _ { 2 } = \\beta _ { 2 } - 3 \\beta _ { 4 } \\xi, \\\\\n",
    "d _ { 2 } = \\beta _ { 3 } + \\beta _ { 4 }\n",
    "$$.\n",
    "\n",
    "\n",
    "**polynomial under consideration is a polynomial over second subdomain**\n",
    "\n",
    "But $f_2$ is a generalized polynomial for $x > \\xi$, so $f$ is as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**in summary and as a segue, define f as piecewise function**\n",
    "\n",
    "In summary we've showed the $f$ can be characterized as $$\n",
    "f(x) = \\begin{array}{ll}\n",
    "      f_1(x) & x\\leq \\xi \\\\\n",
    "      f_2(x) & x > \\xi\\\\\n",
    "\\end{array} \n",
    "$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**consider possibly discontinuous points of f**\n",
    "\n",
    "Polynomials are continuous everywhere, so the only place we have to check is the knot.\n",
    "\n",
    "**show right hand limit is equivalent to left hand limit**\n",
    "\n",
    "Since $f_1$ is a polynomial, and again polynomials are continuous everywhere, the left hand limit of $f(\\xi)$ is $f_1(\\xi)$.  Similarly, the right hand limit of $f(\\xi)$ is $f_2(\\xi)$.  To prove continuity, we must show $f _ { 1 } ( \\xi ) = f _ { 2 } ( \\xi )$.\n",
    "\n",
    "By definition,\n",
    "$$\n",
    "f _ { 1 } ( \\xi ) = \\beta _ { 0 } + \\beta _ { 1 } \\xi + \\beta _ { 2 } \\xi ^ { 2 } + \\beta _ { 3 } \\xi ^ { 3 }\n",
    "$$\n",
    "\n",
    "Simplifying, we get\n",
    "$$\n",
    "f _ { 2 } ( \\xi ) = \\\\\n",
    "\\left( \\beta _ { 0 } - \\beta _ { 4 } \\xi ^ { 3 } \\right) + \\left( \\beta _ { 1 } + 3 \\beta _ { 4 } \\xi ^ { 2 } \\right) \\xi + \\left( \\beta _ { 2 } - 3 \\beta _ { 4 } \\xi \\right) \\xi ^ { 2 } + \\left( \\beta _ { 3 } + \\beta _ { 4 } \\right) \\xi ^ { 3 } = \\\\\n",
    "\\beta _ { 0 } + \\beta _ { 1 } \\xi + \\beta _ { 2 } \\xi ^ { 2 } + \\beta _ { 3 } \\xi ^ { 3 }\n",
    "$$\n",
    "\n",
    "**f(x) is continuous at $\\xi$**\n",
    "\n",
    "Thus, $f$ is continuous at its knot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d - e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we take the necessary derivatives, the arguments are identical to above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alttext](img/p2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it's sufficient to minimize the $\\lambda \\int \\left[ g ^ { ( m ) } ( x ) \\right] ^ { 2 } d x$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a polynomial basis function $g$.  If we square it, it will become symmetric, since multiplying any number by 2 produces an even number.  If we integrate it, it will become anti-symmetric, since adding 1 to an even number will always produce an odd number.\n",
    "\n",
    "Now, we have to find a function on which the difference between any two points is minimized.  This is obviously a *constant* function.  What function $g$ will produce a constant function when squared and integrated?  Only **g = 0**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b - d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same argument as above, we arrive at\n",
    "\n",
    "- g is constant\n",
    "- g is linear\n",
    "- g is quadratic\n",
    "\n",
    ", respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\lambda = 0$ the term with the integral will be zero and it only matters how closely $g$ fits our data.  Thus, **the g that most closely approximates our data** will minimize the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alttext](img/p3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alttext](img/p3a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alttext](img/p5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How will second term (\"regularization term\") select $g$ as to keep the function to a minimum?\n",
    "\n",
    "**two separate sequences of functions converge to two different models of differing complexity**\n",
    "\n",
    "It will attempt to find, for both functions, a sequence of $g$s that converge to the zero function.\n",
    "\n",
    "**quantify flexibility of model**\n",
    "\n",
    "The function $\\hat { g } _ { 1 }$ will select a quadratic polynomial and $\\hat { g } _ { 2 }$ will select a cubic polynomial.\n",
    "\n",
    "**the general behavior of MSE as a function of model flexibility is well known**\n",
    "\n",
    "We know that cubic polynomials are more flexible, and moore flexible models have lower training error, so $\\hat { g } _ { 2 }$ will have smaller training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could potentially use the $C_p$ statistic to derive an estimate for test error from the training error.  Since we expect the *variance* of the cubic model to be higher than that of the *quadratic* model, and we already know the former's flexibility is higher, we would expect $\\hat { \\boldsymbol { g } } _ { 1 }$ to have smaller test RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization term will be zero, and the optimization problem will produce exactly the same sequence of models.  Thus, the two RSSs will be equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
