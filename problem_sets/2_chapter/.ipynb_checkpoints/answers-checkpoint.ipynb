{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/p1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "The flexible method will perform **better** - it has had ample experience to generalize and \n",
    "\n",
    "#### (b)\n",
    "The flexible method will perform **worse** due to the curse of dimensionality\n",
    "\n",
    "#### (c)\n",
    "The flexible method will perform **better** since we need a more complex model.\n",
    "\n",
    "#### (d)\n",
    "The flexible method will perform **worse** since it will learn to fit the error tightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/p2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "regression - n = 500, p = 3\n",
    "#### (b)\n",
    "classification - n = 20, p = 13\n",
    "#### (c)\n",
    "regression - n = 52, p = 52 * 3 = 106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/p3a.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/p3b.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "![alt text](img/a3a.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)\n",
    "- Obviously, as we increase complexity we can get our **training error** all the way down to zero.\n",
    "- The shape of the **test error** curve comes from the bias variance decomposition - note that it's (roughly) the sum of the two.\n",
    "- As we increase model complexity, it's more likely that it will fit the \"coincidence\" of our particular choice in training data, and not the actual pattern.  If it does that, slight differenes in training data can highly affect the differences between new predictions among different classifiers, hence the increase in **variance**.\n",
    "- Almost by definition, a more complex model has less **bias**.\n",
    "- The **Baye's error** is the theoretical lower bound for test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/p5.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the drawing above, we note that a possible advantage is decreasing bias for an inherently more complex data-generating process, and a possible disadvantage is the susceptibility to overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less flexible approaches might be preferred for interpretability reasons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
