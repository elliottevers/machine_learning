{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% Machine Learning Online Class - Exercise 4 Neural Network Learning\n",
    "\n",
    "%  Instructions\n",
    "%  ------------\n",
    "% \n",
    "%  This file contains code that helps you get started on the\n",
    "%  linear exercise. You will need to complete the following functions \n",
    "%  in this exericse:\n",
    "%\n",
    "%     sigmoidGradient.m\n",
    "%     randInitializeWeights.m\n",
    "%     nnCostFunction.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "clear ; close all; clc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_material = /Users/elliottevers/Documents/git-repos.nosync/machine_learning/labs/5/machine-learning-ex4/ex4/\n",
      "Loading and Visualizing Data ...\n",
      "warning: load: '/Users/elliottevers/Documents/git-repos.nosync/machine_learning/labs/5/machine-learning-ex4/ex4/ex4data1.mat' found by searching load path\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANMAAADTCAMAAAAs2dbrAAAAwFBMVEUAAAAEBAQICAgMDAwQEBAU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\n4w+H6oIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_material = '/Users/elliottevers/Documents/git-repos.nosync/machine_learning/labs/5/machine-learning-ex4/ex4/'\n",
    "addpath(dir_material);\n",
    "\n",
    "%% Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400;  % 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;   % 25 hidden units\n",
    "num_labels = 10;          % 10 labels, from 1 to 10   \n",
    "                          % (note that we have mapped \"0\" to label 10)\n",
    "\n",
    "%% =========== Part 1: Loading and Visualizing Data =============\n",
    "%  We start the exercise by first loading and visualizing the dataset. \n",
    "%  You will be working with a dataset that contains handwritten digits.\n",
    "%\n",
    "\n",
    "% Load Training Data\n",
    "fprintf('Loading and Visualizing Data ...\\n')\n",
    "\n",
    "load('ex4data1.mat');\n",
    "m = size(X, 1);\n",
    "\n",
    "% Randomly select 100 data points to display\n",
    "sel = randperm(size(X, 1));\n",
    "sel = sel(1:100);\n",
    "\n",
    "displayData(X(sel, :));\n",
    "\n",
    "% fprintf('Program paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "function [J grad] = nnCostFunction(nn_params, ...\n",
    "                                   input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, ...\n",
    "                                   X, y, lambda)\n",
    "%NNCOSTFUNCTION Implements the neural network cost function for a two layer\n",
    "%neural network which performs classification\n",
    "%   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...\n",
    "%   X, y, lambda) computes the cost and gradient of the neural network. The\n",
    "%   parameters for the neural network are \"unrolled\" into the vector\n",
    "%   nn_params and need to be converted back into the weight matrices. \n",
    "% \n",
    "%   The returned parameter grad should be a \"unrolled\" vector of the\n",
    "%   partial derivatives of the neural network.\n",
    "%\n",
    "\n",
    "% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "% for our 2 layer neural network\n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "% Setup some useful variables\n",
    "m = size(X, 1);\n",
    "         \n",
    "% You need to return the following variables correctly \n",
    "J = 0;\n",
    "Theta1_grad = zeros(size(Theta1));\n",
    "Theta2_grad = zeros(size(Theta2));\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: You should complete the code by working through the\n",
    "%               following parts.\n",
    "%\n",
    "% Part 1: Feedforward the neural network and return the cost in the\n",
    "%         variable J. After implementing Part 1, you can verify that your\n",
    "%         cost function computation is correct by verifying the cost\n",
    "%         computed in ex4.m\n",
    "%\n",
    "% Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "%         Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "%         the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "%         Theta2_grad, respectively. After implementing Part 2, you can check\n",
    "%         that your implementation is correct by running checkNNGradients\n",
    "%\n",
    "%         Note: The vector y passed into the function is a vector of labels\n",
    "%               containing values from 1..K. You need to map this vector into a \n",
    "%               binary vector of 1's and 0's to be used with the neural network\n",
    "%               cost function.\n",
    "%\n",
    "%         Hint: We recommend implementing backpropagation using a for-loop\n",
    "%               over the training examples if you are implementing it for the \n",
    "%               first time.\n",
    "%\n",
    "% Part 3: Implement regularization with the cost function and gradients.\n",
    "%\n",
    "%         Hint: You can implement this around the code for\n",
    "%               backpropagation. That is, you can compute the gradients for\n",
    "%               the regularization separately and then add them to Theta1_grad\n",
    "%               and Theta2_grad from Part 2.\n",
    "%\n",
    "\n",
    "\n",
    "\n",
    "% Part 1: Feedforward the neural network and return the cost in the\n",
    "%         variable J. After implementing Part 1, you can verify that your\n",
    "%         cost function computation is correct by verifying the cost\n",
    "%         computed in ex4.m\n",
    "\n",
    "\n",
    "% feed forward to get ultimate hypothesis\n",
    "bias = ones(size(X,1), 1);\n",
    "\n",
    "input_layer = [bias X];\n",
    "\n",
    "second_layer = sigmoid(input_layer * Theta1');\n",
    "\n",
    "output_layer = sigmoid([bias second_layer] * Theta2');\n",
    "\n",
    "y_vec = zeros(size(X, 1), num_labels);\n",
    "\n",
    "for i=1:size(X, 1)\n",
    "  y_vec(i,y(i))=1;\n",
    "end\n",
    "\n",
    "J = (1/m) * sum(\n",
    "    sum(\n",
    "        (-y_vec) .* log(output_layer) - (1-y_vec) .* log(1-output_layer)\n",
    "    )\n",
    ");\n",
    "\n",
    "% -------------------------------------------------------------\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "% Unroll gradients\n",
    "grad = [Theta1_grad(:) ; Theta2_grad(:)];\n",
    "\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Saved Neural Network Parameters ...\n",
      "warning: load: '/Users/elliottevers/Documents/git-repos.nosync/machine_learning/labs/5/machine-learning-ex4/ex4/ex4weights.mat' found by searching load path\n",
      "\n",
      "Feedforward Using Neural Network ...\n"
     ]
    }
   ],
   "source": [
    "%% ================ Part 2: Loading Parameters ================\n",
    "% In this part of the exercise, we load some pre-initialized \n",
    "% neural network parameters.\n",
    "\n",
    "fprintf('\\nLoading Saved Neural Network Parameters ...\\n')\n",
    "\n",
    "% Load the weights into variables Theta1 and Theta2\n",
    "load('ex4weights.mat');\n",
    "\n",
    "% Unroll parameters \n",
    "nn_params = [Theta1(:) ; Theta2(:)];\n",
    "\n",
    "%% ================ Part 3: Compute Cost (Feedforward) ================\n",
    "%  To the neural network, you should first start by implementing the\n",
    "%  feedforward part of the neural network that returns the cost only. You\n",
    "%  should complete the code in nnCostFunction.m to return cost. After\n",
    "%  implementing the feedforward to compute the cost, you can verify that\n",
    "%  your implementation is correct by verifying that you get the same cost\n",
    "%  as us for the fixed debugging parameters.\n",
    "%\n",
    "%  We suggest implementing the feedforward cost *without* regularization\n",
    "%  first so that it will be easier for you to debug. Later, in part 4, you\n",
    "%  will get to implement the regularized cost.\n",
    "%\n",
    "fprintf('\\nFeedforward Using Neural Network ...\\n')\n",
    "\n",
    "% Weight regularization parameter (we set this to 0 here).\n",
    "lambda = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights): 0.287629 \n",
      "(this value should be about 0.287629)\n"
     ]
    }
   ],
   "source": [
    "% dbstop in nnCostFunction 144\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from ex4weights): %f '...\n",
    "         '\\n(this value should be about 0.287629)\\n'], J);\n",
    "\n",
    "% fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Cost Function (w/ Regularization) ... \n",
      "Cost at parameters (loaded from ex4weights): 0.287629 \n",
      "(this value should be about 0.383770)\n"
     ]
    }
   ],
   "source": [
    "%% =============== Part 4: Implement Regularization ===============\n",
    "%  Once your cost function implementation is correct, you should now\n",
    "%  continue to implement the regularization with the cost.\n",
    "%\n",
    "\n",
    "fprintf('\\nChecking Cost Function (w/ Regularization) ... \\n')\n",
    "\n",
    "% Weight regularization parameter (we set this to 1 here).\n",
    "lambda = 1;\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from ex4weights): %f '...\n",
    "         '\\n(this value should be about 0.383770)\\n'], J);\n",
    "\n",
    "% fprintf('Program paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "function g = sigmoidGradient(z)\n",
    "%SIGMOIDGRADIENT returns the gradient of the sigmoid function\n",
    "%evaluated at z\n",
    "%   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function\n",
    "%   evaluated at z. This should work regardless if z is a matrix or a\n",
    "%   vector. In particular, if z is a vector or matrix, you should return\n",
    "%   the gradient for each element.\n",
    "\n",
    "g = zeros(size(z));\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Compute the gradient of the sigmoid function evaluated at\n",
    "%               each value of z (z can be a matrix, vector or scalar).\n",
    "\n",
    "\n",
    "g = sigmoid(z) .* (1 - sigmoid(z));\n",
    "\n",
    "% =============================================================\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating sigmoid gradient...\n",
      "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n",
      "  \n",
      "0.196612 0.235004 0.250000 0.235004 0.196612 \n",
      "\n",
      "\n",
      "  0.196612 0.235004 0.250000 0.235004 0.196612 \n"
     ]
    }
   ],
   "source": [
    "%% ================ Part 5: Sigmoid Gradient  ================\n",
    "%  Before you start implementing the neural network, you will first\n",
    "%  implement the gradient for the sigmoid function. You should complete the\n",
    "%  code in the sigmoidGradient.m file.\n",
    "%\n",
    "\n",
    "fprintf('\\nEvaluating sigmoid gradient...\\n')\n",
    "\n",
    "g = sigmoidGradient([-1 -0.5 0 0.5 1]);\n",
    "fprintf('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ');\n",
    "fprintf('%f ', g);\n",
    "fprintf('\\n\\n');\n",
    "\n",
    "% fprintf('Program paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Neural Network Parameters ...\n",
      "\n",
      "Checking Backpropagation... \n",
      "  -0.00928   0.00000\n",
      "   0.00890   0.00000\n",
      "  -0.00836   0.00000\n",
      "   0.00763   0.00000\n",
      "  -0.00675   0.00000\n",
      "  -0.00000   0.00000\n",
      "   0.00001   0.00000\n",
      "  -0.00003   0.00000\n",
      "   0.00004   0.00000\n",
      "  -0.00005   0.00000\n",
      "  -0.00018   0.00000\n",
      "   0.00023   0.00000\n",
      "  -0.00029   0.00000\n",
      "   0.00034   0.00000\n",
      "  -0.00038   0.00000\n",
      "  -0.00010   0.00000\n",
      "   0.00012   0.00000\n",
      "  -0.00014   0.00000\n",
      "   0.00015   0.00000\n",
      "  -0.00017   0.00000\n",
      "   0.31454   0.00000\n",
      "   0.11106   0.00000\n",
      "   0.09740   0.00000\n",
      "   0.16409   0.00000\n",
      "   0.05757   0.00000\n",
      "   0.05046   0.00000\n",
      "   0.16457   0.00000\n",
      "   0.05779   0.00000\n",
      "   0.05075   0.00000\n",
      "   0.15834   0.00000\n",
      "   0.05592   0.00000\n",
      "   0.04916   0.00000\n",
      "   0.15113   0.00000\n",
      "   0.05370   0.00000\n",
      "   0.04715   0.00000\n",
      "   0.14957   0.00000\n",
      "   0.05315   0.00000\n",
      "   0.04656   0.00000\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "\n",
      "Relative Difference: 1\n"
     ]
    }
   ],
   "source": [
    "%% ================ Part 6: Initializing Pameters ================\n",
    "%  In this part of the exercise, you will be starting to implment a two\n",
    "%  layer neural network that classifies digits. You will start by\n",
    "%  implementing a function to initialize the weights of the neural network\n",
    "%  (randInitializeWeights.m)\n",
    "\n",
    "fprintf('\\nInitializing Neural Network Parameters ...\\n')\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "% Unroll parameters\n",
    "initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];\n",
    "\n",
    "\n",
    "%% =============== Part 7: Implement Backpropagation ===============\n",
    "%  Once your cost matches up with ours, you should proceed to implement the\n",
    "%  backpropagation algorithm for the neural network. You should add to the\n",
    "%  code you've written in nnCostFunction.m to return the partial\n",
    "%  derivatives of the parameters.\n",
    "%\n",
    "fprintf('\\nChecking Backpropagation... \\n');\n",
    "\n",
    "%  Check gradients by running checkNNGradients\n",
    "checkNNGradients;\n",
    "\n",
    "% fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Backpropagation (w/ Regularization) ... \n",
      "  -0.00928   0.00000\n",
      "   0.00890   0.00000\n",
      "  -0.00836   0.00000\n",
      "   0.00763   0.00000\n",
      "  -0.00675   0.00000\n",
      "  -0.00000   0.00000\n",
      "   0.00001   0.00000\n",
      "  -0.00003   0.00000\n",
      "   0.00004   0.00000\n",
      "  -0.00005   0.00000\n",
      "  -0.00018   0.00000\n",
      "   0.00023   0.00000\n",
      "  -0.00029   0.00000\n",
      "   0.00034   0.00000\n",
      "  -0.00038   0.00000\n",
      "  -0.00010   0.00000\n",
      "   0.00012   0.00000\n",
      "  -0.00014   0.00000\n",
      "   0.00015   0.00000\n",
      "  -0.00017   0.00000\n",
      "   0.31454   0.00000\n",
      "   0.11106   0.00000\n",
      "   0.09740   0.00000\n",
      "   0.16409   0.00000\n",
      "   0.05757   0.00000\n",
      "   0.05046   0.00000\n",
      "   0.16457   0.00000\n",
      "   0.05779   0.00000\n",
      "   0.05075   0.00000\n",
      "   0.15834   0.00000\n",
      "   0.05592   0.00000\n",
      "   0.04916   0.00000\n",
      "   0.15113   0.00000\n",
      "   0.05370   0.00000\n",
      "   0.04715   0.00000\n",
      "   0.14957   0.00000\n",
      "   0.05315   0.00000\n",
      "   0.04656   0.00000\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "\n",
      "Relative Difference: 1\n",
      "\n",
      "\n",
      "Cost at (fixed) debugging parameters (w/ lambda = 3.000000): 0.287629 \n",
      "(for lambda = 3, this value should be about 0.576051)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%% =============== Part 8: Implement Regularization ===============\n",
    "%  Once your backpropagation implementation is correct, you should now\n",
    "%  continue to implement the regularization with the cost and gradient.\n",
    "%\n",
    "\n",
    "fprintf('\\nChecking Backpropagation (w/ Regularization) ... \\n')\n",
    "\n",
    "%  Check gradients by running checkNNGradients\n",
    "lambda = 3;\n",
    "checkNNGradients(lambda);\n",
    "\n",
    "% Also output the costFunction debugging values\n",
    "debug_J  = nnCostFunction(nn_params, input_layer_size, ...\n",
    "                          hidden_layer_size, num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' ...\n",
    "         '\\n(for lambda = 3, this value should be about 0.576051)\\n\\n'], lambda, debug_J);\n",
    "\n",
    "% fprintf('Program paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Neural Network... \n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    fmincg at line 124 column 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%% =================== Part 8: Training NN ===================\n",
    "%  You have now implemented all the code necessary to train a neural \n",
    "%  network. To train your neural network, we will now use \"fmincg\", which\n",
    "%  is a function which works similarly to \"fminunc\". Recall that these\n",
    "%  advanced optimizers are able to train our cost functions efficiently as\n",
    "%  long as we provide them with the gradient computations.\n",
    "%\n",
    "fprintf('\\nTraining Neural Network... \\n')\n",
    "\n",
    "%  After you have completed the assignment, change the MaxIter to a larger\n",
    "%  value to see how more training helps.\n",
    "options = optimset('MaxIter', 50);\n",
    "\n",
    "%  You should also try different values of lambda\n",
    "lambda = 1;\n",
    "\n",
    "% Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = @(p) nnCostFunction(p, ...\n",
    "                                   input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, X, y, lambda);\n",
    "\n",
    "% Now, costFunction is a function that takes in only one argument (the\n",
    "% neural network parameters)\n",
    "[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);\n",
    "\n",
    "% Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "% fprintf('Program paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizing Neural Network... \n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAABqCAMAAABj/zSlAAAAwFBMVEUAAAAEBAQICAgMDAwQEBAU\nFBQYGBgcHBwgICAkJCQoKCgsLCwwMDA0NDQ4ODg8PDxAQEBERERISEhMTExQUFBVVVVZWVldXV1h\nYWFlZWVpaWltbW1xcXF1dXV5eXl9fX2BgYGFhYWJiYmNjY2RkZGVlZWZmZmdnZ2hoaGlpaWqqqqu\nrq6ysrK2tra6urq+vr7CwsLGxsbKysrOzs7S0tLW1tba2tre3t7i4uLm5ubq6uru7u7y8vL29vb6\n+vr///+oYj7dAAAAIUlEQVRo3u3BAQEAAACCIP+vbkhAAQAAAAAAAAAAAACfBixOAAE4c1H1AAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%% ================= Part 9: Visualize Weights =================\n",
    "%  You can now \"visualize\" what the neural network is learning by \n",
    "%  displaying the hidden units to see what features they are capturing in \n",
    "%  the data.\n",
    "\n",
    "fprintf('\\nVisualizing Neural Network... \\n')\n",
    "\n",
    "displayData(Theta1(:, 2:end));\n",
    "\n",
    "% fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "function p = predict(Theta1, Theta2, X)\n",
    "%PREDICT Predict the label of an input given a trained neural network\n",
    "%   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the\n",
    "%   trained weights of a neural network (Theta1, Theta2)\n",
    "\n",
    "% Useful values\n",
    "m = size(X, 1);\n",
    "num_labels = size(Theta2, 1);\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "p = zeros(size(X, 1), 1);\n",
    "\n",
    "h1 = sigmoid([ones(m, 1) X] * Theta1');\n",
    "h2 = sigmoid([ones(m, 1) h1] * Theta2');\n",
    "[dummy, p] = max(h2, [], 2);\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Accuracy: 10.000000\n"
     ]
    }
   ],
   "source": [
    "%% ================= Part 10: Implement Predict =================\n",
    "%  After training the neural network, we would like to use it to predict\n",
    "%  the labels. You will now implement the \"predict\" function to use the\n",
    "%  neural network to predict the labels of the training set. This lets\n",
    "%  you compute the training set accuracy.\n",
    "\n",
    "pred = predict(Theta1, Theta2, X);\n",
    "\n",
    "fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "4.3.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
