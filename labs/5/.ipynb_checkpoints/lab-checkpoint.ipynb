{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% Machine Learning Online Class - Exercise 4 Neural Network Learning\n",
    "\n",
    "%  Instructions\n",
    "%  ------------\n",
    "% \n",
    "%  This file contains code that helps you get started on the\n",
    "%  linear exercise. You will need to complete the following functions \n",
    "%  in this exericse:\n",
    "%\n",
    "%     sigmoidGradient.m\n",
    "%     randInitializeWeights.m\n",
    "%     nnCostFunction.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "clear ; close all; clc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_material = /Users/elliottevers/Documents/git-repos.nosync/machine_learning/labs/5/machine-learning-ex4/ex4/\n",
      "Loading and Visualizing Data ...\n",
      "warning: load: '/Users/elliottevers/Documents/git-repos.nosync/machine_learning/labs/5/machine-learning-ex4/ex4/ex4data1.mat' found by searching load path\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANMAAADTCAMAAAAs2dbrAAAAwFBMVEUAAAAEBAQICAgMDAwQEBAU\nFBQYGBgcHBwgICAkJCQoKCgsLCwwMDA0NDQ4ODg8PDxAQEBERERISEhMTExQUFBVVVVZWVldXV1h\nYWFlZWVpaWltbW1xcXF1dXV5eXl9fX2BgYGFhYWJiYmNjY2RkZGVlZWZmZmdnZ2hoaGlpaWqqqqu\nrq6ysrK2tra6urq+vr7CwsLGxsbKysrOzs7S0tLW1tba2tre3t7i4uLm5ubq6uru7u7y8vL29vb6\n+vr///+oYj7dAAAxWElEQVR42t19aYLlKK5ubqSM53mezeT97+pJeAJ8IiOzblV3vzw/qrMJG/OB\nJCQhiR8//sSf++H3/3fjjYkcP/vZp+23P0WMLr99/Xn2x7vp72CCd4NiYrNn9+qFwRso/sGxn3Qc\nx/w+8at5GcrAHunnQRE3DN3j0WeavSD0P339w+va1w9MhIStEMvGI3ukqaSJ0esxcyQqfWI25nVm\nDJ94k8TfAkP9fvGIO+xiSNSjZyPxi4nvtY1J70F7Pc/D60mFCSCtrM+8fPNfmIQc9V5h6qAD4i48\nNEYatXO9dJl3NxJSCTF3/SZHj9wjJfiu67n22hOvk0JIGj6YiDsJmJLeNYeEaDzPI4QY4/S4nCxM\nboBPdfQ1KYl4nj1e9nGSEtFoM0VIwabACeKt8a5PEVLzHP7rjzK/v0/cbOiXjY2lOVLi90JyANVg\n+/mhYd/5yuRgEUTWLlywLiJEH2co5Hb1+fATPDNt9vQ5hbVOaoaRrNJnUPAdVnm4AjW7BwUMmqpu\nI1k/mDKu6FGKVMcEqwRrVA5MbjjUA30n1zLz03Xv9WUOOrFLuS6CZeeTRw85zL2F6aAf1piYCEmp\nlC/BAbPCpod1iFvQTvVHSDPfE31xnrcsz+IVgIfDguyto02J38ES9Y4PDFhdmCLBEuB7J5xlSe55\nKtnWUF77XsKQ9x+C7KRsDdo72ydqCRnijjAp9CU4SL4X95NAZPKE5Ga0J0RfUbWmwr8xpRQEQcbF\n3umYgPPkDA81Yq8dcsxTt6vlhZVZ1QB+HKsxBSQBSUKcVEwaJhemo3lhgoGK0mJdkvKlftb0xuSO\n7EEPYmQ9/h+wFe89oi2zouieP5i8MPEcl0uRa6QbrIApc1wS0xuTv/LEOTtVcgIbI7oFp8wjZGA6\npkXK7I0pZItrbTDesg/JvgY2puRkkQNTsU/HV3DFtvBmcsf1fRA97ji5GkM4BGUUT64+4dUSRF6N\n7xdcLQ4OP2bj+QniziJQjU65V841CKfTMYWb3AIbE04nfIfoMhKWSebJPtqCA5YpfNCTgNPIUdNX\n8324ZYRbjPOyTn2Hc0UsucvTBxOQIvAC/PwZ6fzAlPDeuVa7lcmBqZLx9V1YyJY8BAEiYvEtTChm\nKpxET9v1iDfLurf5iTiZ4uQbk5syVgVB1fnlzuObzOAznFImcRFCQ0YB7fH4wVTvQhaw56S9wuZe\nZHbOGwgG3HjVOsnkanN7FF03pnaXk2diIk4pKscJ8m7ufI0hpj2d9t6UezBD2/X68X2/neZ5rjy/\nYZrYbiSv46Rkgq9r5eqYIhDcuojZhVjHcQQRe++5QPYyP7Qd0DBweRBTJrpTiQgHFjsa47ZSDq6B\nCcU4h4FtUtDB1wTXJFd209nVWMHmYmBykXFAvYg2oW2vERV0XakQDXKVjgn2vE7DlDIpjk1ryC4q\nga/Ig6Idr0FmOyZvFkPquV5Y00Nju0mnlPvD4iemduecrn0WGLQH6yT3B//JjmK6ZckljE8JVyp2\nuDfiie2CLiWx9ZhSSG2eENQOqHbaeI9uQmCzYoVPgrjnzz4e9EwIztnWhnqfQIn7Xl7i48IU50kS\neA4xZQQpZzElpoAHPslu4WOqyyRnka4wuXGZRd5LrXcGqWOCB5MaBGwVGbYGqEugLwwr54WuR0RF\nVZVZ4Bg6LAxUPqO6ZYSjKcsafi/0LB2aZKO2TKYBFOeWbuY4H1RwZ9p3g57hOT90HftDYHxEUXjZ\nOlefjvMyavDZtr15/Bs7VzcLtJ30w5PGn37aJ8yKJqOez3z6kNb+syEZput3mP6VRqA1j/z9179v\n/DN///F1+tcb/yuYPrhT/iVM5PFT/GOfIs5b7qHx7Qe6k+Lr18knuUOIPU4lYF+YlEWaZ2/jn9yd\nfPt9QsxtAzfJIny5Hty4XTbRueZG/EHEuUr7TGJry4am0vCbgKpUlIlry3J4MKzYSktH7xXMAk/N\nVIEakzFT1wxqn/LSZm6MkYIBq5kAF6SaSc7Y5htPEk0eG5guz4f2ZGs6KUCJXuXOeluHdbNpWedg\nrkxM6QrGkxO0QkY6JiAe3Ex9V7eeiw10FlZpQFHZV28afUYSHSSea7i9SD6tia+5na4eyLiZahBJ\nJl6H3aOwwFbP5DgwYdmEYGFmnucnk76mxImYnCKvWKWyrJ/pc9uxjf1KayRuxVkV+0mq2+4lF1Tm\nFiYwnPM6Mxx0qCkO0zacFohOEB4bTPdgSlewvTR2RF/E6IMSWRiYoLnGiZ8KQw3LNjkVIwWl+Xbn\nHHRXS75O6SiDp7EUa2S6EmH+qBiKvT2bz0GFVHCx8yHUiacBNduf98W318nveWr2yeYQuTa+lxlM\nVJkSJxaWf88dG8dxu0lXwYnP0ADg6yZEfssItF06IacwcMfltspIQEfPUllwkLObIqYguZ7E+RDr\n1ClP5v1ksBaO46+ydiwRA+bgaCi2wTajI9IpePAYADFt0dJdrHVKtqHb9k2nE5JL2lRZBKvVahYE\nkMkuxtQDwqzuxYNuU0uxVc6JNAC68KqNDzemas3BAIhWET195qvnxAN7nBT3Mq0sMkinRaMbhkt7\nzcfhJhFahZ3FTyDHe5nml72sOvDqFDXmaZ9ukiBuDiJmQmdksKya9egvmUNMTAEHU0fgT4qtvtEf\nflwn5/WDCXB2tO5eDiqn2lt9KyAhA4ohTsJE7OhMBkJ3vdygmiz3p5I4VWcQL/LBIE/Bo1wPndjB\nSFftItNFXLbmrkF7JAM+5AvYcMuszjZ+aDsASD82OTemcJZjGqzly4k+sdjAlNPYIUE5rVT31qvd\nRmwvH4s3oakfjiaXonON3/hhOSirfOXHrzUzXT2Yra2hMgAbx3HktbI/HfY/1HZzQnIyMV6Y1JmM\n6xQitiSkU+yN2WfCyrRZx2xuLJswpYLFNqYAm5CEzJEW7DQzT0zoASLKp9G4FvqozwyV4VjmUd5i\nm2RzD/ypSC86SeUhHtKProUpoJNnrr3bb9uQOxkzOY+E2+nJNWmPJijN+86w3VN+jf3kp1NVqln9\ncs46jT0o5DP67LkkgX2B06FM0wrknq0bDb3l+ACBEJo8ehy8OY7Fecq9t46nh1vT9/oxAKN40BQR\nEC/sPv65MCk6rXhOPgx/eZ3pkUDwx0zHg6Ni5hLEBusDS2y7Y2c7c9bacuacKxpulYU+kzQOqDyJ\n7MbkdWud1muoie14k4Nn94qCe6xsbRvPmR7if0bqLULDpFxbWT0ORfz2wS+1qW3DrqUtk76i+V5Z\nVOq3QMnt3pmYoNOonqdbxB0uqtH70CtxPVPEqZdB5L9PfkFDqD3r++QQFS/01GRHWKfe+4wpGkO7\nz0MXiC1MtrYPm2vX+Fqnhl1gk4RXd7VHXk9+Orb/wkni35T7rTPnY5/Pt35i5+oH6Xavr0bz4Z89\n+UUjrLRL/v7rRuOf+fs/Tsr/YOM3mD7Svut+YHKdH40+f8Wd8tk9aYzg+3GaZwBfPguqauB+YEgw\nTKxPoTjzg8A3d1L8g+1Oub6tG1XwryD0gw+OF9gkQs/ok3zwBeFToX9Nya1HKDe0LXlgd+W1+xKc\nUZ73dWwa31G9UM5p9Zy7H39wJ1Ho+taFCfTB06iC2cj7ge5U6kcI19OgRRnHMjD8pMjN2BTiN2Au\nbKWnyz3iFvM0dZFjrD5alf1mHcbD0oGKAzo3a/SJThmb+24QNCQmQSZsbw0d0it8JXhLFlyY/Gao\nshx+tp2rzt91LQqGNK4Mz9gNg77gc11P57nYaRMGA53qZqaFb+q7XhBbGg/Gd+DxF+eSFbePBUyg\nEsjOSblmVB1YheTGOp0KKOgN1L/CO05+cIhNe6jJXSM9VrThMNJesFyPYwHKw4/v5YMJdC20zh23\nkaatgZMpDY2HuOkGiLYiTY+TMY2iYE3Ww3mjYRqvA81rSkKq9Gcwr5rHTD5+WWfTHlie5zKdmEoJ\nOoQ38H6ZdU+csgovYwsx4cm3QgiGmixMWyMfVlppygWpuORd5KOU2CxMwHw0s1gn2KSo9UGFi/oy\nTmPqGMMHe3OxFVv05mSa6U06CfwRz3ElSn2diOclCy8fvRytoeg0Ps+TXm2kQJa3QQ+SYJNrftqr\nzR5qziS/HEXvW8LQG6Vs9SmJVp6rU1qwCT2DSgOw3l9RaRGVPdEx1buK0irZrDOZW02LuM60T0yF\nKA8lnsrVFJxA/xErNdt9PzX3F6Z633vbfsJl2h+/DUBaRBtkWZaHtSwdV5voZF2itxe6k0yPHlRR\nZlPkNWIOdDsvZJKustVjqMAiAHpCnW2V9HH5nmOPeftgGtmlPWOEho4pmxmvzU0TzMRdbo9FjvEW\nGKWHYVSc65yHQTTBSzN1wH5rLWXd6zjbNBfJgT1Ofb9hua7DkmTZ6dhVlb5O5ADlFA9FA6b23jEb\neWK69puMMTOKhyRU6jFMMA10HMemLNNO9hpBwB/W4KWw4CeYdUKOBAGTUjtaI9g+isroYNi5xM+q\ngTLGdVdepmSuU9HgwdRO5BS70aasK2W9Hm6TVIyWSd3BLhY/s4++FE/p8O6kSWg1Ti3k4h5+zC5u\n0iRkuIiF01hjMq9t1AJc3osLkwr3CYJCJrffjCS9khHrqBOPSOBfgZ9UQADJ4eFqKvV6RLnp+IFX\n5T472kgvpY4kQveCQ7eb89L3COnlGZ+Lc3sBBdvTy9isuRy7Tq1TyDqd9q5lJfEZSXTgB7Pf8UaR\nGgxB53leVy7levr3yKjERrY97phzUDE8VdgUpXrueaY7EoONBqf7Rp+S7V4mUPkuKt14QpxaJtc4\nQRhkSmMabzeuocOSgD2KCJAzb/JZaK4PEJrbDj8pt7m+PBcklXPXraAu2KFygImHnzBFdNWpFCz8\nfe26rp+NkJFK8IRY64T+MeVTqR5Mk1jbul532xd2TeC4aa552AZ2Wmrn/vCvbBiatslC79bLgZ7F\nvi9NYjM5KAGidj9gAi2mt6RZzWGixGJgqvfbuabx0wq6mjfIe3NHx+LMdjk3r/i94w3YCmc9hiq4\nFfirg2PWnvAURdFBeGPUh3+bGfY6BbV9eAhfCi9L5Qaqf/x+stn6ot9645zQOwZwPWnaT8Qrazv4\n0+zVjDi5qNRs019/NZ5/sBo/2YSffSynz10nXTsMxrIJT3PLbHwP6n+78c/8/RfmlLwCu/7hdfoX\nh/+F54T4rSUjTNbV0X9w5nz19ftD/yYm0IXC8PZ9aMMfqHXUdMiu17GIEqm2Z113kJroPfNcg1zB\nfrbjSQss/A6T7QuDrXHBHXo1DUU8sPBffpts5vsQmiMlaIKK3gqaII5fNvkrkoIE1XQs/2Fr4FCy\nDpSe6XRIaHMal+1Uey9M5BPxXIcDP46GQixNmuQz0x0/qMcNNj8RN6nqbTcjTvDjvM9pbQyJeM02\nV5nluQAjewMb5rAz0SZ0/TgfxS7XeRVbaHZQo49ITIc3SBn4V2CRZSoRL+1nsd2n6RgJAYoyrH3K\nK8MfkLCXwkRw9leZmX265Ta4Tm6YqSScl9R1XoFNqeBdxZXKd9i5Jd/3DdbTAbUx1TsAtUcwzrgK\noP1BLlYOsrLrYt1YSJpuYWDY7NopbUgLR52Tiljv02nnT+cabn9HSJ+k67Zyn1BHLTRLy5tbnNpm\nck16SoYcRq8smwNTWDW5j3Mfc+OYFYxkWvhB2Iwqs4UkPiZlzRuldFtp/mi7MClgvkWVkPUD1MMT\nWuIk3AiCAkyF85JmqKp3piPRydk8xzgn4xOVRgIOi08qmlssrqImmEotOe3ck7sxyOShfczdYZlD\n/OF0vDglvDJvU1slvhOz7g4G8OnOQYVzun0xrNc1d9yEWv41d8qhT0tGhL0oTFkO+naf4odJsejq\nWrwsfoIhBRrjksPdPR/ZOoYsxzyaUUueS1VsN1ir50QTzJIJQg+pOaP0Sc2odvTDgOnLK6Iv87bV\nHR9tP/Aaez3NTQnZPKH519djlgWlwnRQ1D3IaVmKx3bHxfDjoq6rXnTeC5OTi/3x72EcwPAX8ZvL\njPlxnjFiL7VYHn4icZ8qSHtluj4GIXeR6L4D5XsopraeDQkJ5oB1cIxiPDgYuDX91cSXWlwUyPBc\ncTKIMnYGt+h2rjvuq5ac4C5iKpsZdojr+5p/ZdRD2Q8xuOyDtSQBFSObrbQcd4JFzSf9dXcUl/X3\nUEm/+UoHd+fJcLk65d4/7EiCCWN/pg7l8/EpDROwD9+bh05V+B3+mJHPqATSuco6l5J8twJRQJSJ\nwk3YpO95sCZ78RfpChMTdc6/PimW9ZELB7TaGJ7AdOy1FBLgDDFEnleKtdl2dVL/YMJMFmmkpBG3\naOqeC1PE4lDNQJB7kd/+COBOp9ByWXFI2zx56WoEP7ojOwMN8vDGlPFQ0XnD9C0TJFbUNc/5E3Cb\nnPOkhY3VidrK1zEREGR4AmEMynH+KoWw8qi9XrQvixw+JWszuh8YgcfIZroPHma9DmZ6xw1cmPYu\ncN24XtpbxMH2htGMbslLY5nrzKfpnW6C2gqVgiuyO0OMH0zRKi8nuu4cXqXlMXbqvX8FgmAcxmwB\nRa6b266jLNEweXXhuokZSQEEwXa6ghJT6VF5wLRtt4rG1Lcmf+p0qQ9CoizLQ6czfWEkWqR8Radg\nsJYoiEVRxTsUgyQbDW3tgPgT+pjayBTb5G1WwHY5CDEW5pNuzdg6GOHhhMzdOPrGhyzL/cGEg/9A\nUSBjVjs6hRhRE+ectqJ8u1OUqRH82hm1G2D6ldUY+L6pLKOqX38IFzL7vNbJzYr3s8TNu+y1eO9B\nAZdoW6bpovmF1+9Hv2/84OH5EpNLjOl/ev2l6BTiRp+jeNz/SuOf+fvvzum/s05/owM9aOTfHKke\n3vLDaPvHMYGQKqfUsjTx96upRmeEyetJ0xuCx1V+Q+ms5UuoZGDvrhOiv/4kED0+FmLPPrnVcOvk\n2a1WcemR+kid3Ap6/yijiAqEqapUi3g5D2pCsx4L6PXLRie+PbYG8dthnWdzzzv/kpvnGvBEkpVl\n6hkqy4nG8wyHBnEyvnRjbvcK2oDobfQuee15Xl7P27pt6vj1x/WcMlSbXiczPGuq4hAUp8K5N41R\npSfLNbQxgV7Z6HoEGNgLPLmLJdViGfw8dKNiWIVY9Fwlkq6N775WH/PL78jha/hDaueUqZCHMcak\n9/rBlGKqwXG2pK9TjPqi02uYwo13YdrzVwYOAZ1t0DCh1Sr52taTELfCCIaJXEYwysdmkZpBTxJa\nfgprA9NE2GHf3YCZalZO2YjaFvFmdlXvwAVCRZm0cnZtcYAq5+ENOlriNABCYW9M/hWrdWIK1n2M\nfYwpZoeO80OpRZJvTQ768iAMD1dLyOkVMzA9J8q3PwQsL6fmPjHtpy1xcAQ35+NRMMYG+VR2xBYc\nWPdoeDhPWSTh8o4gg3FdGXEKk5PLLVCHxE4tz0mB9ZibWB3+JlI/EK3x6NUJ66FPNVPtiC2wbMJ4\nAUk4d86B/1o8AoqdP3GtggGpdrRpYyZyPzSGj0fMVBY6QQJFgrrNUxM9mv9GXg1wAlNZAI4/yisM\nCMjj8CWRZtcOrgOGtkdOGRM0JVq8EVglg5XxHdCxbPe26gtTxIXttmmCi/gw79M8IePO+c06RAkn\nf9nNcCEgPAy6t+JtUIs28mrQJqOZ7/rRdLln3Vu+wqIyzUkRisohFe8CP1ZJJ/dIq8vFoc1p1C07\n6+vY0yP9goaxzteolCRCBbegm+QWB0ijlE71eNneqhEThYpd1KUUvokpZXZNjGABmdfNXJ4H5Qbv\n6QGAClPC8OwbLFYNk9vv6ztA3HHWzjhXgCXeRB8aigDG+ClE+D/5RSUpcFfN9juSBRv9Iof/lKUX\nS26ljdaP9XftT2FHAQ+dJbdTQyq56KUqMNelRvvPKWdfw+TNsnurLMRV/uVn+Fg4KLY2LRKvjM4D\nF2vdlCeVAHPxyk2Bc5Z7RdER0rkX25uYgmmNbUww7XkJ29ELU8CfHFH3cCW2SpTFs5G2Goj7Of31\n6ArsufxrIMIdx1ZYvChwMsGi+1AIpe7oRBvWEGsutxMIRoFs7ziwhrNxMODeAVQPpoNOgNaZGSh5\nTo3x/VEssL13m5bRh1Dl7H7A1FDPxERpW5ZJYHmYCMoJI7hk2Nd228ekE8sdXOJvso+jpB1g1zTz\ntBJaa2tv6LAk5NI8V6nlZHIJJsphfZ+t1k+10Cvdkw+YDv+9RmZKXWFiNI/PUJxfJYcOTA2wEm19\nJ0ju7Q29vzvKEYk5mqbDl8ZfYvLGvdQxhezOu9TUsLQsy9DcXn32BLVpmJzejDci5U6HMktqZh81\nDbcT9+AnLy7z0AqFIG45S9oPVeqbemm1JfdnXphAJFY6RTVPhK9h0D+h6Nc6ze/8K3xyCC1Mc6QO\nxALXWqfaikkkRqmBu9ELA9c+tiekq8iXmNCxkOiY5tV/j9T43YP6bFZkFpNdh0Kv8ycjEOXLD9l+\nr+fr+pOWTWgdMscR+TVMXzQSz/2Fkf7TjX/m7x+aKeL/clIWsdnxH1+nf6hXpxh/ERNJ284uKPKL\nfptXCNi/iolE22xXLTpCDizrERM+9v0K236AOijSbHeGnUPvll0/Fb+KiZArXejleHp3oAna63Xv\nKpn6NBI3SYPQxawoIw8h3eZNyNJQBNwon3dRmOjdJLM0wwDVXNFZ0fW2g+jGFGXFNE1damY1aeVp\ndaBh19e6VfZRNwJzjm90Wbe10Q8vYSf3w3yjgfGhWmz9thr5GhjKn5qY/JUvm9jX1Nxz3TjP6/xC\nevvCIsoZxyqnU/DYOm42iSm2yBw2zBb06MWoUplyFtkrik4XwbZtrkLjQFaRRMEbY/H8NCCNjWmy\ndH2YzNj1aiaZUZ02XTgeUrPS0c+fUGPYsiSJK/4c3IN+JcXKVjuWIdr2JnD8RDPow5lnVx7TAzTh\nrIz8wLWqdxx9T1ol2RPp2FicV4gttOhZOROWXfOFYYkVsXXtLLaj3MNtP21AzMAkrXyKbILCuSWe\nP1rJ5cFKwUwjNdMKddT7oavqfmA0EydNkzEwuc6wBSYmPYnCvYiXxm9ZSpxwfQx6PHXlTQirmrEz\nYOXEFDQVDqiRi1bnpOZg0TiZmbHgNAIsILcXT8IFkPnRGxBrHZAroy5c0cocz8AkUxgTZ9CjHtA1\nW8x20VYsXRJ/WCfixPSOtwFLC4Uo/rKl0jCdNQUzQbVIDndBu9vJeKkp9litNYrzXvaPL+ywhQ9O\nl+uVUYdpc5xTIVmtxcZc+l64GRE3YKrvsjCjQ6B1sjChYzBI09Br9965MZ0fB6Y04lgOOqUs1eMj\n2iX2vKjfuR7K7mNJSbBhsifWzhlV6hPaNyvnd+ajlxa+53npJq9oK6wrXbTt2Lc1UH+su2hQwM+v\nmMSJp7qEwvh4imVeGJPTg2lVuj6mkegy4nh+kyXR5Um4innmcqO6gxBz+qQ0amI4KkxNJfslfPSs\nLEFY6OHM7CA5bEzi/E2J7krzQtKI3I5JnLRqou7hlzs7kJeIIeEit6EoqmFcllRfJ1cVoF6tFE+/\nGrcuCRfDQQjmmv2phInK96GDftKrO13Pr5t/YqpgQGzqJ3G5GS6KQhlRi+J7TJLlYRgP4ti0VWPY\ncZUpxrk8Il6ePXcScrVLwR7kPxnRm+jxMw16FcWzbRjxc7hX79eVPEvYVdiaBLMUnG5cHfI/jKuK\nvMTz+qK9RlqYqp0VYVJv8kjHPBrduJmntoyaU2hf/OTOks3jK0Ra/WnczOOGyQr+RKnVgDzgbHjC\nO4A9c5XXES9HoqHip3DEGCZO9drAJIMp8su7ZJMm4AsbE0yKxHrHaxbqTnTH90Gvy86M52vPTaho\nXPeTZoxKgh786VS7ViD2XhIPf+4jtmEqeBXHac/klj5AMZi3y424ZeDDbeTSrvPxAZOrsqPh0T58\n66Uqj0cVQT78sHhYM/lfnJMCQ9SPu5/ErPtUpEVXIw9M/Y7svIshNDRDLUD40kyR7Mf4/XWS2pjw\nUCopok9nn2pDANq/MLnDvnws6HKsTLfcC41l8/0vnzQI0q9WUPbq2DU0nveTrrpG432iiH9o32UW\nyad6kuefMlE/65TfGVGfng3Xp9J60IZfP2k0fjoj/ozps+fENZwnP/nQ8zjWlNL1iK+fBRHw7Pnk\n14u0/Ocbj7qbf+bvO/zkHy+w+m83arrRx/gQkPKVR77vlejc8/33r0f/7vDt1404jgeT+/EKIbSs\n4u8xEVAgQTsKDz/Jd4PCj/tJU2o67FfD/0wl6sYhT6/04Bd9XwRG7ChqvIuk4WvXI93+Kl/4zNNj\n5XecrnTfeflp9tXD2jyB2jGuUrS3thsdtR7ewydeHr62AlAn6pHvnTYloKxgYkmt6Uawk21yqmnz\nwpQ+5vMzKWqMnu6P8Oe5Xea6fNXcPcYaVXdyt7JM2S62+qoxgxNXf8aEQbGdhQn+b7aCZUC7xwCI\nNtEkUdypw5ULkz/z3PXmzO4VTKPKDkRB4ytqVq7loMTb6JPnJMDEhFrKUjr3lLij2JrIfRiC+EuP\nmnmcvV4Pt6sO2zOj7iT4WAWBZtRM4ijUMasLKk5MERa/xIo2lpfjTL+5MQEhJ21XzoyL5/QN1BCl\nWBm60UkjPl4tw7ogv5PwSb6mgSFNSID3f7hkGl6YWkEtZ5IbVRJrZxLdAKjyY/J6HVOMKSlqBUz/\nXru3+uxj2UrQ4OhUBr1WDNWdaWllXB+QJozlSNnset6NKV5Sy18OmGp1T1Zp14eFJZ0LM7kazGzl\nf7DZwT28De6DKdzU7TZOuhpnnxFjYAYkd3wEIfO+9EmIBWVF+NBe0m+UdqZuhvY41iNJ+Rz4d0UQ\noPL8Ly+J9EEB7Y3KdfCqD+tRUeplrUEB5AJL3LxYXP03UYfvt61Ry9F3IlDkjZIiLXBTNLM7fQsd\nQ76yLNZdK9QBHYGhxhbd/FKFlAIFyY9T9xYHDe+GlVEjXwPM9sz5q3nf5laIxQjBwvNgTAQ0jSpy\nzcDUEt0mJMC4NeONcfYbMRqkVHBZPkty/Kp91rPo1S+lm15+I+Od6zgxncKivesCYMUkuvblwnQJ\nCUTKm3iiL4rqpVnZi/jNxqTokmd7ORz9ij/682qY23b3Sr7zVM89JE4l24AuLdsCbUmIOvZdtDnF\nyCT8Q65X73Am2bVVtciuyl19SqLIc5x8N/0BXrVi2R+LooLrEiiNR8O4GHZxzz1xq6GrqixO4gYs\nQvfhJ+x1YEwaFVKxUklcsnajibXr+CuPtUwxtw5V7BeZNK8ZScd5pPuWR0/OMbBmiNtanEfCONdA\neVruqYkJb6GZ3uc/mH3zCEMSYKEvIbCWDN9WtO0e2utYFvTcIPOIsmKRcsssAe9NUo+4ASViyVRU\nXKSHYiDXsznQBbxT7GyZBpi9comseXKyFyZ/FokhSy/K97e74BYJd1EmWUPl1uR5EmrrRCKgG8e/\nHP7HoEJM46WVdREk0C3LzOiUYGVzigswGyVz3WXTLM0fyoXNpSp+spWuJbaJjQk9Z6Onf4h4oYuF\n+tpNtOSmvU7OTTvT6Y75uTEFjM/1KA3Od9Nx6kwho9QVkTvW9AXlyrepLrlRVy6otFybS+o387rM\njU9sEQd2t3WJTcxUveTndacQ8zgBkckxuqcEM6i3ddACsx5+yichptrK63GMOx7O2RNPltpDZn7a\nYuKyQVFW2MJJPOgCfhv0uD0ZnjiMqelM250E3YJRcWurb29YxMhz9CdvTKC9B569Pxu/o4OUtx9S\naNAmCN4lCOzXXctO09VVrzIx+eMcWB9CewgLHbjW5m6ZKt/ZhDYmb+lde/a193/Bevyq0f46eS+z\n7bb5zs79te8fVSh/Y6T/jcY/8/f7k/JriV4/ayTO/+n1b9fp589+IF6w/ZPvOe9njSQpyMcn346X\nv/MhA9PboaEHpp2NKkQ5+MVPffau+uvyERNoc4FrHF8R367zTs7olJ8I2AcTHquUsWOpYVE7lKbD\nm8DOuoQfMb18YeTTROMxffkJE978uXgGJqeZDQ8Pmjl+mhZ15pnj1A4WDEw53WlvpiWFmEcw68VI\nwXRi4ccL0Y4DEyO8wx/G6p0ZH/B3lX31fsav6xVvTDW10tWHZaUbE7I1kuCjYnx0uGd/8mt1hMie\njAW8hIj3PZO8uy+CxNspXte4nB37RdX3mXYNp9fLpwLfM6e1ncd8Pb7o6Sa4wqBMMOMak1WMVep7\nsKC6Pe4kVHLOrtS5W4+IZslXsCLV+eE5/BrLbrrJqDwAl1HVYdHSD+cqbr6Asj8vj9cOy4b20r74\nmwTb5H5aZrfZtRI1BysnUsfkdJgVgCbpqZieT0ar6JN0HY3aj1iOZudFWNeLvO47VRdxjOj38QZx\nRT0gJpXs8fZylKINfd/zg2dKJC+Sc5za7Nd3jUjT7TUc5XlulSHviBMbmEiKkJxwEa1hZWPtB8ep\n9HrLqIIOEl2oxGmluAIcgk0cB7xYDLg2MKEDwjPV1ZxP/nW3pnrd7zjPnViIs6TMTRCs/6jDgjT1\ndOvNXyfH6wxMB8um65Jo8T6ogmICBLkujkBMxIuwuAwanup+o2udKnG4JTH5Ua4mJr+lePvFwzox\n28JbyP1Arp2lrJS7vVSJmveTnfIavJY5ZXcQ2DHSTNTVtls3R8Ncdvs+9s1T81ZFO6Fwmq9CFYjJ\n9TIuB3Lc2cLP1EX0157ZBoipezDtjRMvdBiVsXv0SkK6c8qH57YefwWBsy7TjMfs/LnRixA6esE0\nx6YGjwEn1rVIlRQ7ozAubUVV5tLStz2fNB9HhbESTsav5DVFeyBwFmXUVOLImvih8ii388NIe+nN\nJZnYooXGJJDVeR+2l4fJOhRpTm+TkoTy/GE0xpLf6wQLFzv1Lm3vKvpNjMN4WPl9ilrj5mjcnbLA\nhR23ploMU7CMDm5EWn3Yo16u47jhKJ8CRU4FOA/XCelV5uc10YNceAndXutE4o3NdeyHFX+q0bhx\nVvV12zRSdKkWO0qSNXC6udEK0SKkSnKrHgpwaOs5BibAGR+lNJ2U666slrcTCO3LSaH4adrHICxw\nI3qKdZN45V2ArrOk50usMUREJcvc/Dh/V58K6xkrKNP24ecr2CjBQsL6lu1vdVCwTS/rjaF7UgtQ\nuCYPfuPe3pjQ+VueByWZhgnWvh3bJOb6OnmjZOsmd0nzUHPQpRys5KZtKd+e+9TUBGIZaj5pvmXi\nx0mRhe+dFA8mei/Ww9oqvgHrlw8/wSpRucaf9ryr1te59stxxkKcYNuM/CecvYhr9ZZVpNcu2dSG\nejQyFrjF2edL6tlK4DiPRgW8L81PdSv0UBihr0kzTaUmzUAW789FRQYm4OlHiyI+61SRQC/bWGwN\nCWVEpcXDEj8viiK2bxrGA6E0TbPLc/R8inyoPvg5wAGmehZlaH/fONfAK+vyz9fthFTHBEK7L+q2\nnffpdXiI1kKt60bmRRTa7H+8KNn4fdsIZGnXNLSeJO5TisVeZl/PaMM7K2bG+Zq4n4hc5Ja+95sj\n/Q1Mr8PD15Mvx4/xso7+ax8LqBc3Pf/rmP5TjUSvH/En/jTYxPkYC/6/tiTfNeqYQHp3IfnNDvTV\n/551/tOY0OAxVJbf6fWA47/vFfqF10/xatcyPQTv38j+0v0REd03U7Uk5NO2cflu9J3U9eNqGJbF\nulfog8P5FdQHIquqqiJNr/DJ++tuWJbJG6hikQ+HDZf1pmHyR9Tj7FPa106KAsaPy2oY88fKj/pF\nXc0jzNqTR7yL3afreXrhFxJO67LMG92mwsjNDnsq5WpdjoLH5KNKWX704kfE+7F29qkKEIhKlxHE\nAz0iIJbvAFT7GGvmYnbObabDJs7nrq5GwbP7Fhqsgxs1i9BC7hFO3IDKxXJd3/Pwn77f7ka+OaiV\na8+ZGcEFE8D7fKyd+An9JFkbR2EQJPXIRv08FyDJwdWYnIQj2+USWnqEuoNm6ZuGK4PnxFR3WFw3\nXY4clnNKCizUT4W4a6fAc/OkbhqW+i3T5xyXfNJvbAHjkyeVWEw715tZBjps8awTug12xrZt4wxD\nwx4fC1hFeBn3k3uHILus7OPYvCQbK8CFnpfNTKuyjzUKvaRn4lHW0ZwXaxMl/LnsSMUyS8nZxs3c\nO9e9b+V5PpTUSbCJwtheYPwlSJNKv+/Sp3yuyyxPoyPV6sSEXh+5hk6YVcU5UjA1VLGGfqdWNVE8\n+J74pB/GYxSQstKzp7oUkG6MWU1CNyvwM30W5WyvLMU65ZPl3QTzYRRiMAwAp8J4B3/Tig45HU8c\nXWe6MAXLvsZxPYnjIh+FCWv4ErDWpJ6jie9lM18rw8fjNvQ40l9CnckJXqOiO1KJl2ZoZ1ZPzdmL\n9NfhVbzQX1QaxqCZSjBFUxTP7PGvgVW1eOSNCYNjeNVyrPHErxozIYalhJvsV913QLxy4lKYUUww\nJZLOTVZuojApilhXaRxfxlQYK3VwWDxbMyWh5NM4cb2kBTDpvCxCu3ibJOhIcKOirM/akSemcJVN\nDz30m7zzRfxGbMPEW5LIRxyBOaQcJ9y4tR2jOPDg22lk55iYOhG9MVX3TVfX8GuRvexc2FwKoHP7\n3jvPL4WWDuiUsgCBxwVd2eF5ODHlQnRcTvV45s2qRjeftqVUhTLKW8hEbG3SoN67p1ct4qeTpblO\nAZ/fZwAezJwpzULWvjZCoqoRAVsv9hHGFS5z+rd2ztiUBL4XzHet7wPTUbdGPFXInp0sp7K/76gL\nPOIkVMuBwfsOYxU26V03oj3Dz/bupdwAN8nRXLyOP6U37rXPUnR90VcUT6pfoYPUWOeBEthOqrLC\nTkzBsWmIznI9nL1Ei5zIQzwxFc0zUpIs21bCnlnA9mpxvjfzV9EjVaLJIsh+1bWWo9EH3odZkpNv\nYZpEZtTiQRrBKneY+5o+mEgyTNNUxZ+DAfDP9Xkll0PCgm2pplwQzJ/hlHHBR8tNQGI+vTjf64U0\nb65Ggd9Fj254Yprltgi52Yle4Ta+A6vidWqa6TjtuPfcM9f9sxJ8efaJl+TtJgejvC0pJhBFy9RV\nie0FB2bI3py/SmaVoMbKWGzq4ddoGk+FgnxO7T7zD0WHsFT+NE+1Gb9n/uzGC1PAGHzFjGNBHc73\nfe99uwk6el7lZc+qP9ZW5Eb1uAxAxHdZOnUmOBf+68lu/1RIiTyp+b/njyBenutVpFzd5fpBg3eq\nvSUfgL4aTe/JD6PNfj1qozcmzZfzuz4Wc+g/e/Jck8R/0R40BuSXXv/c+H10xp/5+/2Z+qJR3Z3w\n91//3KfzNyJevsX0Odrrg4REiW8V6vgY1vay3X/ydeJV9a89+QnTFR3yMv7dpN9SeyPGKE77SVBS\n1kTHhHezVMGbyQnKSffTRvgaqZPKxtydyRde6A+YUDWMyzILzE0TdzPOaGrt5GjrtnZAayZLc/ZB\ni+NifRVLiJp5XbfGN0XcR1canlObh4dRUobOd2t/2k9usSwUDI31lezjx/G2WkVa/FbKM8z27tWn\npbkkePy4tryw0nJSKtZhmHf9cpZ7SMcW84y048Z9nwQUzZ1W4dtFUtdNZslyrKYq1zQuuVn7El1O\nfiWsUrSYMz+d91beQOvVNacP+gSboLcvzEwrPJ8vjBuZ/TSIo7icqKqDq6lB1wHx9aQ3bxOTwryi\nAO2xfd9Fmxh7Lqh/G8ang7njm7ue12yiMS/IqJgYc3/aG/1Jn9rBzE62b74TL9a1D2pRvG15gjZU\nQUgwkvd1ljqmsxSuMU9uQOKGciNPC03MqWlnxtvAiMV2fQyddXsdE+7Z275zvQQ2liDGurh4C6se\nxZQIM5RckR7WCbFrWKsoouta1vNJLyiGpoj9QRq3KqV8+3jImzO9nAoa2dVhKzIlzQy5h/ca6bSH\nIWB906zGeTL0oMqfJFzDRMg82VnspJV10dZZYtTxPf9SGUDP8+zCuvZ8lPVLHKB0jjYtuAUvyuzP\n4UechYbP0g3S+jpruxYvjYnzV7FphXdIfbjwbExsIJ4Ryq1yzASYIWwuPZPzc9F+2J+AADYj2z8T\nyysSnbj1NK5i1fwREbuyDfBgutL9e9GEhY+swtbYI+l1TD5TLki0BHJ9TrehW9fW1ymqFWIdp2UD\nsy7UonhIRKcP4X9ada6j0akxqMQiXXV/rbguGD99duvVHfGW9cnhx3gfvi7XHRtnoyJ+n2lZiiQ/\nUl0wRcMzMHVRnA1aGWSsqM7Tv8CwLyf1zoXJPbjmA0GaF+NgLZO4mcZOzy0A5Fgsa39EjJNhneKT\neoPtwYTXLweB7/VavenzMW/QnGmuMyhnI7kroF+YaP0XbPGd1MpfhB2WF8UziAYDEy9MyX4UrvVc\ngyArOVp3j9WSMcHlbmyEmK6QFMNzxzQIfLy5l6ggjVWc9SOA38f8AKCFzMB+n/hBmPRSj111esxt\nJW7OZjM+Ytlgf3dcTRgS5N6tjAPPLWRJNM7HOKCk1U91UDotVtywg/V56GhVgDuUOO+pVoa178RY\nZ3kNu45Ka1KYnGbH2F7M5HlqnTs5BrdsXBjFdEgs1hCvw9usbJWQyrEZmFHlBK/6FWydF1VJ98cl\nCQY3bGY6hYYw2qht6alTCRAxlZ2fi+VXVr38eAxKAKZ0Dfm9P2H0zhx7SUPl8nAp8QcgXKqytTRM\n7rxvwyYHKykKvYHjMreB+X2vXCl8bHkid4HJ6YzFjYwMVdAMP/gsg25qEzv7KoiDOOk1N6oKtYvg\nF1w8+uPHQcx05vtaGiW4vTTLIs/edTB4bCtfgusLtd4NoqRIff38q2EbVvnTpT4QafNAevp8wmU0\ngphWvktevNVFU9/DQnt0rX1LCdW71LgkfKcXW7+n8TkkfeSe79vzVO2d6bf5SZ8g+eg2WCGN5pOX\nvvfZfvvQ62d3yleYPg3K/hDG3/2kGozdaCzJTzD91qD+6UY8TXV/HdP3jX/i7/8Bph7KdGI2HLkA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_material = '/Users/elliottevers/Documents/git-repos.nosync/machine_learning/labs/5/machine-learning-ex4/ex4/'\n",
    "addpath(dir_material);\n",
    "\n",
    "%% Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400;  % 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;   % 25 hidden units\n",
    "num_labels = 10;          % 10 labels, from 1 to 10   \n",
    "                          % (note that we have mapped \"0\" to label 10)\n",
    "\n",
    "%% =========== Part 1: Loading and Visualizing Data =============\n",
    "%  We start the exercise by first loading and visualizing the dataset. \n",
    "%  You will be working with a dataset that contains handwritten digits.\n",
    "%\n",
    "\n",
    "% Load Training Data\n",
    "fprintf('Loading and Visualizing Data ...\\n')\n",
    "\n",
    "load('ex4data1.mat');\n",
    "m = size(X, 1);\n",
    "\n",
    "% Randomly select 100 data points to display\n",
    "sel = randperm(size(X, 1));\n",
    "sel = sel(1:100);\n",
    "\n",
    "displayData(X(sel, :));\n",
    "\n",
    "% fprintf('Program paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Saved Neural Network Parameters ...\n",
      "warning: load: '/Users/elliottevers/Documents/git-repos.nosync/machine_learning/labs/5/machine-learning-ex4/ex4/ex4weights.mat' found by searching load path\n",
      "\n",
      "Feedforward Using Neural Network ...\n"
     ]
    }
   ],
   "source": [
    "%% ================ Part 2: Loading Parameters ================\n",
    "% In this part of the exercise, we load some pre-initialized \n",
    "% neural network parameters.\n",
    "\n",
    "fprintf('\\nLoading Saved Neural Network Parameters ...\\n')\n",
    "\n",
    "% Load the weights into variables Theta1 and Theta2\n",
    "load('ex4weights.mat');\n",
    "\n",
    "% Unroll parameters \n",
    "nn_params = [Theta1(:) ; Theta2(:)];\n",
    "\n",
    "%% ================ Part 3: Compute Cost (Feedforward) ================\n",
    "%  To the neural network, you should first start by implementing the\n",
    "%  feedforward part of the neural network that returns the cost only. You\n",
    "%  should complete the code in nnCostFunction.m to return cost. After\n",
    "%  implementing the feedforward to compute the cost, you can verify that\n",
    "%  your implementation is correct by verifying that you get the same cost\n",
    "%  as us for the fixed debugging parameters.\n",
    "%\n",
    "%  We suggest implementing the feedforward cost *without* regularization\n",
    "%  first so that it will be easier for you to debug. Later, in part 4, you\n",
    "%  will get to implement the regularized cost.\n",
    "%\n",
    "fprintf('\\nFeedforward Using Neural Network ...\\n')\n",
    "\n",
    "% Weight regularization parameter (we set this to 0 here).\n",
    "lambda = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "function [J grad] = nnCostFunction(nn_params, ...\n",
    "                                   input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, ...\n",
    "                                   X, y, lambda)\n",
    "%NNCOSTFUNCTION Implements the neural network cost function for a two layer\n",
    "%neural network which performs classification\n",
    "%   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...\n",
    "%   X, y, lambda) computes the cost and gradient of the neural network. The\n",
    "%   parameters for the neural network are \"unrolled\" into the vector\n",
    "%   nn_params and need to be converted back into the weight matrices. \n",
    "% \n",
    "%   The returned parameter grad should be a \"unrolled\" vector of the\n",
    "%   partial derivatives of the neural network.\n",
    "%\n",
    "\n",
    "% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "% for our 2 layer neural network\n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "% Setup some useful variables\n",
    "m = size(X, 1);\n",
    "         \n",
    "% You need to return the following variables correctly \n",
    "J = 0;\n",
    "Theta1_grad = zeros(size(Theta1));\n",
    "Theta2_grad = zeros(size(Theta2));\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: You should complete the code by working through the\n",
    "%               following parts.\n",
    "%\n",
    "% Part 1: Feedforward the neural network and return the cost in the\n",
    "%         variable J. After implementing Part 1, you can verify that your\n",
    "%         cost function computation is correct by verifying the cost\n",
    "%         computed in ex4.m\n",
    "%\n",
    "% Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "%         Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "%         the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "%         Theta2_grad, respectively. After implementing Part 2, you can check\n",
    "%         that your implementation is correct by running checkNNGradients\n",
    "%\n",
    "%         Note: The vector y passed into the function is a vector of labels\n",
    "%               containing values from 1..K. You need to map this vector into a \n",
    "%               binary vector of 1's and 0's to be used with the neural network\n",
    "%               cost function.\n",
    "%\n",
    "%         Hint: We recommend implementing backpropagation using a for-loop\n",
    "%               over the training examples if you are implementing it for the \n",
    "%               first time.\n",
    "%\n",
    "% Part 3: Implement regularization with the cost function and gradients.\n",
    "%\n",
    "%         Hint: You can implement this around the code for\n",
    "%               backpropagation. That is, you can compute the gradients for\n",
    "%               the regularization separately and then add them to Theta1_grad\n",
    "%               and Theta2_grad from Part 2.\n",
    "%\n",
    "\n",
    "\n",
    "\n",
    "% Part 1: Feedforward the neural network and return the cost in the\n",
    "%         variable J. After implementing Part 1, you can verify that your\n",
    "%         cost function computation is correct by verifying the cost\n",
    "%         computed in ex4.m\n",
    "\n",
    "\n",
    "% feed forward to get final hypothesis\n",
    "bias = ones(size(X,1), 1);\n",
    "\n",
    "input_layer = [bias X];\n",
    "\n",
    "second_layer = sigmoid(input_layer * Theta1');\n",
    "\n",
    "output_layer = sigmoid([bias second_layer] * Theta2');\n",
    "\n",
    "y_vec = zeros(size(X, 1), num_labels);\n",
    "\n",
    "for i=1:size(X, 1)\n",
    "  y_vec(i,y(i))=1;\n",
    "end\n",
    "\n",
    "J = (1/m) * sum(\n",
    "    sum(\n",
    "        (-y_vec) .* log(output_layer) - (1-y_vec) .* log(1-output_layer)\n",
    "    )\n",
    ");\n",
    "\n",
    "Theta1_without_bias = Theta1(:,2:size(Theta1,2));\n",
    "\n",
    "Theta2_without_bias = Theta2(:,2:size(Theta2,2));\n",
    "\n",
    "term_reg = (lambda/(2*m))  * (\n",
    "    sum(sum(Theta1_without_bias .^ 2 )) + sum(sum(Theta2_without_bias .^ 2 ))\n",
    ");\n",
    "\n",
    "J = J + term_reg;\n",
    "\n",
    "for t=1:m\n",
    "    bias = ones(size(X(t, :),1), 1);\n",
    "\n",
    "    input_layer_activations_with_bias = [bias X(t, :)];\n",
    "\n",
    "    second_layer_activations = sigmoid(input_layer_activations_with_bias * Theta1');\n",
    "\n",
    "    output_layer_activations = sigmoid([bias second_layer_activations] * Theta2');\n",
    "\n",
    "    y_vec = zeros(size(X, 1), num_labels);\n",
    "\n",
    "    for i=1:size(X, 1)\n",
    "      y_vec(i,y(i))=1;\n",
    "    end\n",
    "\n",
    "    delta_3 = output_layer_activations - y_vec(t, :);\n",
    "\n",
    "    second_layer_activations_with_bias =[1; second_layer_activations'];\n",
    "\n",
    "    delta_2 = (Theta2' * delta_3') .* sigmoidGradient(second_layer_activations_with_bias);\n",
    "\n",
    "    Theta2_grad = Theta2_grad + delta_3' * second_layer_activations_with_bias';\n",
    "    \n",
    "    Theta1_grad = Theta1_grad + delta_2(2:end) * input_layer_activations_with_bias;\n",
    "    \n",
    "end;\n",
    "\n",
    "Theta2_grad = (1/m) * Theta2_grad;\n",
    "\n",
    "Theta1_grad = (1/m) * Theta1_grad;\n",
    "\n",
    "Theta1_grad(:, 2:end) = Theta1_grad(:, 2:end) + ((lambda/m) * Theta1(:, 2:end)); \n",
    "\n",
    "Theta2_grad(:, 2:end) = Theta2_grad(:, 2:end) + ((lambda/m) * Theta2(:, 2:end));\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "% Unroll gradients\n",
    "grad = [Theta1_grad(:) ; Theta2_grad(:)];\n",
    "\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights): 0.383770 \n",
      "(this value should be about 0.287629)\n"
     ]
    }
   ],
   "source": [
    "% dbstop in nnCostFunction 98\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from ex4weights): %f '...\n",
    "         '\\n(this value should be about 0.287629)\\n'], J);\n",
    "\n",
    "% fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Cost Function (w/ Regularization) ... \n",
      "Cost at parameters (loaded from ex4weights): 0.383770 \n",
      "(this value should be about 0.383770)\n"
     ]
    }
   ],
   "source": [
    "%% =============== Part 4: Implement Regularization ===============\n",
    "%  Once your cost function implementation is correct, you should now\n",
    "%  continue to implement the regularization with the cost.\n",
    "%\n",
    "\n",
    "fprintf('\\nChecking Cost Function (w/ Regularization) ... \\n')\n",
    "\n",
    "% Weight regularization parameter (we set this to 1 here).\n",
    "lambda = 1;\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from ex4weights): %f '...\n",
    "         '\\n(this value should be about 0.383770)\\n'], J);\n",
    "\n",
    "% fprintf('Program paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "function g = sigmoidGradient(z)\n",
    "%SIGMOIDGRADIENT returns the gradient of the sigmoid function\n",
    "%evaluated at z\n",
    "%   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function\n",
    "%   evaluated at z. This should work regardless if z is a matrix or a\n",
    "%   vector. In particular, if z is a vector or matrix, you should return\n",
    "%   the gradient for each element.\n",
    "\n",
    "g = zeros(size(z));\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Compute the gradient of the sigmoid function evaluated at\n",
    "%               each value of z (z can be a matrix, vector or scalar).\n",
    "\n",
    "\n",
    "g = sigmoid(z) .* (1 - sigmoid(z));\n",
    "\n",
    "% =============================================================\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating sigmoid gradient...\n",
      "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n",
      "  \n",
      "0.196612 0.235004 0.250000 0.235004 0.196612 \n",
      "\n",
      "\n",
      "  0.196612 0.235004 0.250000 0.235004 0.196612 \n"
     ]
    }
   ],
   "source": [
    "%% ================ Part 5: Sigmoid Gradient  ================\n",
    "%  Before you start implementing the neural network, you will first\n",
    "%  implement the gradient for the sigmoid function. You should complete the\n",
    "%  code in the sigmoidGradient.m file.\n",
    "%\n",
    "\n",
    "fprintf('\\nEvaluating sigmoid gradient...\\n')\n",
    "\n",
    "g = sigmoidGradient([-1 -0.5 0 0.5 1]);\n",
    "fprintf('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ');\n",
    "fprintf('%f ', g);\n",
    "fprintf('\\n\\n');\n",
    "\n",
    "% fprintf('Program paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "function W = randInitializeWeights(L_in, L_out)\n",
    "%RANDINITIALIZEWEIGHTS Randomly initialize the weights of a layer with L_in\n",
    "%incoming connections and L_out outgoing connections\n",
    "%   W = RANDINITIALIZEWEIGHTS(L_in, L_out) randomly initializes the weights \n",
    "%   of a layer with L_in incoming connections and L_out outgoing \n",
    "%   connections. \n",
    "%\n",
    "%   Note that W should be set to a matrix of size(L_out, 1 + L_in) as\n",
    "%   the first column of W handles the \"bias\" terms\n",
    "%\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "W = zeros(L_out, 1 + L_in);\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Initialize W randomly so that we break the symmetry while\n",
    "%               training the neural network.\n",
    "%\n",
    "% Note: The first column of W corresponds to the parameters for the bias unit\n",
    "%\n",
    "\n",
    "epsilon_init = 0.12;\n",
    "\n",
    "W = rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init;\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Neural Network Parameters ...\n",
      "\n",
      "Checking Backpropagation... \n",
      "  -9.2783e-03  -8.6919e-03\n",
      "   8.8991e-03   8.3357e-03\n",
      "  -8.3601e-03  -7.8523e-03\n",
      "   7.6281e-03   7.2103e-03\n",
      "  -6.7480e-03  -6.3907e-03\n",
      "  -3.0498e-06  -2.9547e-06\n",
      "   1.4287e-05   1.3282e-05\n",
      "  -2.5938e-05  -2.3918e-05\n",
      "   3.6988e-05   3.4642e-05\n",
      "  -4.6876e-05  -4.4598e-05\n",
      "  -1.7506e-04  -1.6409e-04\n",
      "   2.3315e-04   2.1835e-04\n",
      "  -2.8747e-04  -2.6973e-04\n",
      "   3.3532e-04   3.1660e-04\n",
      "  -3.7622e-04  -3.5624e-04\n",
      "  -9.6266e-05  -9.0135e-05\n",
      "   1.1798e-04   1.1059e-04\n",
      "  -1.3715e-04  -1.2911e-04\n",
      "   1.5325e-04   1.4497e-04\n",
      "  -1.6656e-04  -1.5750e-04\n",
      "   3.1454e-01   3.1454e-01\n",
      "   1.1106e-01   1.1106e-01\n",
      "   9.7401e-02   9.7401e-02\n",
      "   1.6409e-01   1.6409e-01\n",
      "   5.7574e-02   5.7574e-02\n",
      "   5.0458e-02   5.0458e-02\n",
      "   1.6457e-01   1.6457e-01\n",
      "   5.7787e-02   5.7787e-02\n",
      "   5.0753e-02   5.0753e-02\n",
      "   1.5834e-01   1.5834e-01\n",
      "   5.5924e-02   5.5924e-02\n",
      "   4.9162e-02   4.9162e-02\n",
      "   1.5113e-01   1.5113e-01\n",
      "   5.3697e-02   5.3697e-02\n",
      "   4.7146e-02   4.7146e-02\n",
      "   1.4957e-01   1.4957e-01\n",
      "   5.3154e-02   5.3154e-02\n",
      "   4.6560e-02   4.6560e-02\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "\n",
      "Relative Difference: 0.00105872\n"
     ]
    }
   ],
   "source": [
    "%% ================ Part 6: Initializing Pameters ================\n",
    "%  In this part of the exercise, you will be starting to implment a two\n",
    "%  layer neural network that classifies digits. You will start by\n",
    "%  implementing a function to initialize the weights of the neural network\n",
    "%  (randInitializeWeights.m)\n",
    "\n",
    "fprintf('\\nInitializing Neural Network Parameters ...\\n')\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "% Unroll parameters\n",
    "initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];\n",
    "\n",
    "\n",
    "%% =============== Part 7: Implement Backpropagation ===============\n",
    "%  Once your cost matches up with ours, you should proceed to implement the\n",
    "%  backpropagation algorithm for the neural network. You should add to the\n",
    "%  code you've written in nnCostFunction.m to return the partial\n",
    "%  derivatives of the parameters.\n",
    "%\n",
    "fprintf('\\nChecking Backpropagation... \\n');\n",
    "\n",
    "%  Check gradients by running checkNNGradients\n",
    "checkNNGradients;\n",
    "\n",
    "% fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Backpropagation (w/ Regularization) ... \n",
      "  -9.2783e-03  -8.6919e-03\n",
      "   8.8991e-03   8.3357e-03\n",
      "  -8.3601e-03  -7.8523e-03\n",
      "   7.6281e-03   7.2103e-03\n",
      "  -6.7480e-03  -6.3907e-03\n",
      "  -1.6768e-02  -1.6768e-02\n",
      "   3.9433e-02   3.9432e-02\n",
      "   5.9336e-02   5.9338e-02\n",
      "   2.4764e-02   2.4762e-02\n",
      "  -3.2688e-02  -3.2686e-02\n",
      "  -6.0174e-02  -6.0163e-02\n",
      "  -3.1961e-02  -3.1976e-02\n",
      "   2.4923e-02   2.4940e-02\n",
      "   5.9772e-02   5.9753e-02\n",
      "   3.8641e-02   3.8661e-02\n",
      "  -1.7370e-02  -1.7364e-02\n",
      "  -5.7566e-02  -5.7573e-02\n",
      "  -4.5196e-02  -4.5188e-02\n",
      "   9.1459e-03   9.1376e-03\n",
      "   5.4610e-02   5.4619e-02\n",
      "   3.1454e-01   3.1454e-01\n",
      "   1.1106e-01   1.1106e-01\n",
      "   9.7401e-02   9.7401e-02\n",
      "   1.1868e-01   1.1868e-01\n",
      "   3.8193e-05   3.8193e-05\n",
      "   3.3693e-02   3.3693e-02\n",
      "   2.0399e-01   2.0399e-01\n",
      "   1.1715e-01   1.1715e-01\n",
      "   7.5480e-02   7.5480e-02\n",
      "   1.2570e-01   1.2570e-01\n",
      "  -4.0759e-03  -4.0759e-03\n",
      "   1.6968e-02   1.6968e-02\n",
      "   1.7634e-01   1.7634e-01\n",
      "   1.1313e-01   1.1313e-01\n",
      "   8.6163e-02   8.6163e-02\n",
      "   1.3229e-01   1.3229e-01\n",
      "  -4.5296e-03  -4.5296e-03\n",
      "   1.5005e-03   1.5005e-03\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "\n",
      "Relative Difference: 0.00099577\n",
      "\n",
      "\n",
      "Cost at (fixed) debugging parameters (w/ lambda = 3.000000): 0.576051 \n",
      "(for lambda = 3, this value should be about 0.576051)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%% =============== Part 8: Implement Regularization ===============\n",
    "%  Once your backpropagation implementation is correct, you should now\n",
    "%  continue to implement the regularization with the cost and gradient.\n",
    "%\n",
    "\n",
    "fprintf('\\nChecking Backpropagation (w/ Regularization) ... \\n')\n",
    "\n",
    "%  Check gradients by running checkNNGradients\n",
    "lambda = 3;\n",
    "checkNNGradients(lambda);\n",
    "\n",
    "% Also output the costFunction debugging values\n",
    "debug_J  = nnCostFunction(nn_params, input_layer_size, ...\n",
    "                          hidden_layer_size, num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' ...\n",
    "         '\\n(for lambda = 3, this value should be about 0.576051)\\n\\n'], lambda, debug_J);\n",
    "\n",
    "% fprintf('Program paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Neural Network... \n",
      "Iteration     1 | Cost: 3.325075e+00\n",
      "Restarting kernel...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m[Errno 9] Bad file descriptor\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%% =================== Part 8: Training NN ===================\n",
    "%  You have now implemented all the code necessary to train a neural \n",
    "%  network. To train your neural network, we will now use \"fmincg\", which\n",
    "%  is a function which works similarly to \"fminunc\". Recall that these\n",
    "%  advanced optimizers are able to train our cost functions efficiently as\n",
    "%  long as we provide them with the gradient computations.\n",
    "%\n",
    "fprintf('\\nTraining Neural Network... \\n')\n",
    "\n",
    "%  After you have completed the assignment, change the MaxIter to a larger\n",
    "%  value to see how more training helps.\n",
    "options = optimset('MaxIter', 50);\n",
    "\n",
    "%  You should also try different values of lambda\n",
    "lambda = 1;\n",
    "\n",
    "% Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = @(p) nnCostFunction(p, ...\n",
    "                                   input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, X, y, lambda);\n",
    "\n",
    "% Now, costFunction is a function that takes in only one argument (the\n",
    "% neural network parameters)\n",
    "[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);\n",
    "\n",
    "% Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "% fprintf('Program paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizing Neural Network... \n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n",
      "warning: division by zero\n",
      "warning: called from\n",
      "    displayData at line 42 column 76\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAABqCAMAAABj/zSlAAAAwFBMVEUAAAAEBAQICAgMDAwQEBAU\nFBQYGBgcHBwgICAkJCQoKCgsLCwwMDA0NDQ4ODg8PDxAQEBERERISEhMTExQUFBVVVVZWVldXV1h\nYWFlZWVpaWltbW1xcXF1dXV5eXl9fX2BgYGFhYWJiYmNjY2RkZGVlZWZmZmdnZ2hoaGlpaWqqqqu\nrq6ysrK2tra6urq+vr7CwsLGxsbKysrOzs7S0tLW1tba2tre3t7i4uLm5ubq6uru7u7y8vL29vb6\n+vr///+oYj7dAAAAIUlEQVRo3u3BAQEAAACCIP+vbkhAAQAAAAAAAAAAAACfBixOAAE4c1H1AAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%% ================= Part 9: Visualize Weights =================\n",
    "%  You can now \"visualize\" what the neural network is learning by \n",
    "%  displaying the hidden units to see what features they are capturing in \n",
    "%  the data.\n",
    "\n",
    "fprintf('\\nVisualizing Neural Network... \\n')\n",
    "\n",
    "displayData(Theta1(:, 2:end));\n",
    "\n",
    "% fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "function p = predict(Theta1, Theta2, X)\n",
    "%PREDICT Predict the label of an input given a trained neural network\n",
    "%   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the\n",
    "%   trained weights of a neural network (Theta1, Theta2)\n",
    "\n",
    "% Useful values\n",
    "m = size(X, 1);\n",
    "num_labels = size(Theta2, 1);\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "p = zeros(size(X, 1), 1);\n",
    "\n",
    "h1 = sigmoid([ones(m, 1) X] * Theta1');\n",
    "h2 = sigmoid([ones(m, 1) h1] * Theta2');\n",
    "[dummy, p] = max(h2, [], 2);\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Accuracy: 10.000000\n"
     ]
    }
   ],
   "source": [
    "%% ================= Part 10: Implement Predict =================\n",
    "%  After training the neural network, we would like to use it to predict\n",
    "%  the labels. You will now implement the \"predict\" function to use the\n",
    "%  neural network to predict the labels of the training set. This lets\n",
    "%  you compute the training set accuracy.\n",
    "\n",
    "pred = predict(Theta1, Theta2, X);\n",
    "% \n",
    "fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "4.3.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
