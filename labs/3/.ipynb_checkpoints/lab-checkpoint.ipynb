{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% Machine Learning Online Class - Exercise 2: Logistic Regression\n",
    "%\n",
    "%  Instructions\n",
    "%  ------------\n",
    "% \n",
    "%  This file contains code that helps you get started on the logistic\n",
    "%  regression exercise. You will need to complete the following functions \n",
    "%  in this exericse:\n",
    "%\n",
    "%     sigmoid.m\n",
    "%     costFunction.m\n",
    "%     predict.m\n",
    "%     costFunctionReg.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "clear ; close all; clc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%plot gnuplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code_dir = /Users/elliottevers/Documents/git-repos.nosync/machine_learning/labs/3/machine-learning-ex2/ex2/\n"
     ]
    }
   ],
   "source": [
    "code_dir = \"/Users/elliottevers/Documents/git-repos.nosync/machine_learning/labs/3/machine-learning-ex2/ex2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "function plotData(X, y)\n",
    "%PLOTDATA Plots the data points X and y into a new figure \n",
    "%   PLOTDATA(x,y) plots the data points with + for the positive examples\n",
    "%   and o for the negative examples. X is assumed to be a Mx2 matrix.\n",
    "\n",
    "% Create New Figure\n",
    "figure; hold on;\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Plot the positive and negative examples on a\n",
    "%               2D plot, using the option 'k+' for the positive\n",
    "%               examples and 'ko' for the negative examples.\n",
    "%\n",
    "\n",
    "% Find Indices of Positive and Negative Examples\n",
    "pos = find(y==1); neg = find(y == 0);\n",
    "% Plot Examples\n",
    "plot(X(pos, 1), X(pos, 2), 'k+','LineWidth', 2, ...\n",
    "     'MarkerSize', 7);\n",
    "plot(X(neg, 1), X(neg, 2), 'ko', 'MarkerFaceColor', 'y', ...\n",
    "     'MarkerSize', 7);\n",
    "\n",
    "\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "\n",
    "\n",
    "hold off;\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting data with + indicating (y = 1) examples and o indicating (y = 0) examples.\n"
     ]
    }
   ],
   "source": [
    "%% Load Data\n",
    "%  The first two columns contains the exam scores and the third column\n",
    "%  contains the label.\n",
    "\n",
    "data = load(strcat(code_dir, 'ex2data1.txt'));\n",
    "X = data(:, [1, 2]); y = data(:, 3);\n",
    "\n",
    "%% ==================== Part 1: Plotting ====================\n",
    "%  We start the exercise by first plotting the data to understand the \n",
    "%  the problem we are working with.\n",
    "\n",
    "fprintf(['Plotting data with + indicating (y = 1) examples and o ' ...\n",
    "         'indicating (y = 0) examples.\\n']);\n",
    "\n",
    "plotData(X, y);\n",
    "\n",
    "% Put some labels \n",
    "hold on;\n",
    "% Labels and Legend\n",
    "xlabel('Exam 1 score')\n",
    "ylabel('Exam 2 score')\n",
    "\n",
    "% Specified in plot order\n",
    "legend('Admitted', 'Not admitted')\n",
    "hold off;\n",
    "\n",
    "% fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def sigmoid(\n",
    "    Real z\n",
    ") -> Real:\n",
    "    return 1/(1+e^(-1 * z))\n",
    "\n",
    "\n",
    "\n",
    "def hypothesis(\n",
    "    Real x,\n",
    "    List[Coefficient] theta\n",
    ") -> Real:\n",
    "    List[Real] pre_sigmoid= [\n",
    "        x*coefficient\n",
    "        for coefficient\n",
    "        in coefficients\n",
    "    ]\n",
    "    return sigmoid(\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "function g = sigmoid(z)\n",
    "%SIGMOID Compute sigmoid function\n",
    "%   g = SIGMOID(z) computes the sigmoid of z.\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "g = zeros(size(z));\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Compute the sigmoid of each value of z (z can be a matrix,\n",
    "%               vector or scalar).\n",
    "\n",
    "\n",
    "f = @(x) 1 / (1 + e^(-1 * x));\n",
    "\n",
    "g = arrayfun(\n",
    "    f,\n",
    "    z\n",
    ");\n",
    "\n",
    "% =============================================================\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def logistic_loss(\n",
    "    Data data,\n",
    "    Callable[\n",
    "        List[Real],\n",
    "        List[Real]\n",
    "    ] hypothesis\n",
    ") -> Real:\n",
    "    \n",
    "    for response, predictor in data:\n",
    "        Real left = (0 - response) * log(0 + hypothesis(predictor))\n",
    "        Real right = (1 - response) * log(1 - hypothesis(predictor))\n",
    "        \n",
    "        sum = sum([\n",
    "            difference(\n",
    "                left, right\n",
    "            )\n",
    "            for predictor, response\n",
    "            in data\n",
    "        ])\n",
    "        \n",
    "    quotient = 1/m\n",
    "    return product(\n",
    "        quotient,\n",
    "        sum\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "function [J, grad] = costFunction(theta, X, y)\n",
    "%COSTFUNCTION Compute cost and gradient for logistic regression\n",
    "%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the\n",
    "%   parameter for logistic regression and the gradient of the cost\n",
    "%   w.r.t. to the parameters.\n",
    "\n",
    "% Initialize some useful values\n",
    "m = length(y); % number of training examples\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "J = 0;\n",
    "grad = zeros(size(theta));\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Compute the cost of a particular choice of theta.\n",
    "%               You should set J to the cost.\n",
    "%               Compute the partial derivatives and set grad to the partial\n",
    "%               derivatives of the cost w.r.t. each parameter in theta\n",
    "%\n",
    "% Note: grad should have the same dimensions as theta\n",
    "%\n",
    "\n",
    "theta_i_l = 1:size(theta, 1);\n",
    "data_i_l = 1:size([X y], 1);\n",
    "hypothesis = @(X) sigmoid(X * theta);\n",
    "predictors = X;\n",
    "response = y;\n",
    "\n",
    "%%% COST %%%\n",
    "\n",
    "left = @(data_i_a) (-1 * response(data_i_a) * log(hypothesis(predictors(data_i_a, :))));\n",
    "\n",
    "right = @(data_i_a) ((1 - response(data_i_a)) * log(1 - hypothesis(predictors(data_i_a, :))));\n",
    "\n",
    "term_cost = @(data_i_a) left(data_i_a) - right(data_i_a);\n",
    "\n",
    "sum_cost = sum(\n",
    "    arrayfun(\n",
    "        term_cost,\n",
    "        data_i_l\n",
    "    )\n",
    ");\n",
    "\n",
    "J = (1/m) * sum_cost;\n",
    "\n",
    "\n",
    "\n",
    "%%% GRADIENT %%%\n",
    "\n",
    "term_grad = @(theta_i_a, data_i_a)...\n",
    "    @(data_i_a)...\n",
    "        (hypothesis(predictors(data_i_a, :)) - response(data_i_a)) * predictors(data_i_a, theta_i_a)...\n",
    ";\n",
    "\n",
    "sum_grad = @(theta_i_a)...\n",
    "    sum(...\n",
    "        arrayfun(...\n",
    "            term_grad(theta_i_a),...\n",
    "            data_i_l...\n",
    "        )...\n",
    "    )...\n",
    ";\n",
    "\n",
    "product = @(theta_i_a) (1/m) * sum_grad(theta_i_a);\n",
    "\n",
    "grad = arrayfun(\n",
    "    product,\n",
    "    theta_i_l\n",
    ");\n",
    "\n",
    "% =============================================================\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% ============ Part 2: Compute Cost and Gradient ============\n",
    "%  In this part of the exercise, you will implement the cost and gradient\n",
    "%  for logistic regression. You neeed to complete the code in \n",
    "%  costFunction.m\n",
    "\n",
    "%  Setup the data matrix appropriately, and add ones for the intercept term\n",
    "[m, n] = size(X);\n",
    "\n",
    "% Add intercept term to x and X_test\n",
    "X = [ones(m, 1) X];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial theta (zeros): 0.693147\n",
      "Expected cost (approx): 0.693\n",
      "Gradient at initial theta (zeros): \n",
      " -0.100000 \n",
      " -12.009217 \n",
      " -11.262842 \n",
      "Expected gradients (approx):\n",
      " -0.1000\n",
      " -12.0092\n",
      " -11.2628\n",
      "\n",
      "Cost at test theta: 0.218330\n",
      "Expected cost (approx): 0.218\n",
      "Gradient at test theta: \n",
      " 0.042903 \n",
      " 2.566234 \n",
      " 2.646797 \n",
      "Expected gradients (approx):\n",
      " 0.043\n",
      " 2.566\n",
      " 2.647\n"
     ]
    }
   ],
   "source": [
    "% Initialize fitting parameters\n",
    "initial_theta = zeros(n + 1, 1);\n",
    "\n",
    "% Compute and display initial cost and gradient\n",
    "[cost, grad] = costFunction(initial_theta, X, y);\n",
    "\n",
    "fprintf('Cost at initial theta (zeros): %f\\n', cost);\n",
    "fprintf('Expected cost (approx): 0.693\\n');\n",
    "fprintf('Gradient at initial theta (zeros): \\n');\n",
    "fprintf(' %f \\n', grad);\n",
    "fprintf('Expected gradients (approx):\\n -0.1000\\n -12.0092\\n -11.2628\\n');\n",
    "\n",
    "% Compute and display cost and gradient with non-zero theta\n",
    "test_theta = [-24; 0.2; 0.2];\n",
    "[cost, grad] = costFunction(test_theta, X, y);\n",
    "\n",
    "fprintf('\\nCost at test theta: %f\\n', cost);\n",
    "fprintf('Expected cost (approx): 0.218\\n');\n",
    "fprintf('Gradient at test theta: \\n');\n",
    "fprintf(' %f \\n', grad);\n",
    "fprintf('Expected gradients (approx):\\n 0.043\\n 2.566\\n 2.647\\n');\n",
    "\n",
    "% fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at theta found by fminunc: 0.203498\n",
      "Expected cost (approx): 0.203\n",
      "theta: \n",
      " -25.161272 \n",
      " 0.206233 \n",
      " 0.201470 \n",
      "Expected theta (approx):\n",
      " -25.161\n",
      " 0.206\n",
      " 0.201\n"
     ]
    }
   ],
   "source": [
    "%% ============= Part 3: Optimizing using fminunc  =============\n",
    "%  In this exercise, you will use a built-in function (fminunc) to find the\n",
    "%  optimal parameters theta.\n",
    "\n",
    "%  Set options for fminunc\n",
    "options = optimset('GradObj', 'on', 'MaxIter', 400);\n",
    "\n",
    "%  Run fminunc to obtain the optimal theta\n",
    "%  This function will return theta and the cost \n",
    "[theta, cost] = ...\n",
    "\tfminunc(@(t)(costFunction(t, X, y)), initial_theta, options);\n",
    "\n",
    "% Print theta to screen\n",
    "fprintf('Cost at theta found by fminunc: %f\\n', cost);\n",
    "fprintf('Expected cost (approx): 0.203\\n');\n",
    "fprintf('theta: \\n');\n",
    "fprintf(' %f \\n', theta);\n",
    "fprintf('Expected theta (approx):\\n');\n",
    "fprintf(' -25.161\\n 0.206\\n 0.201\\n');\n",
    "\n",
    "% Plot Boundary\n",
    "plotDecisionBoundary(theta, X, y);\n",
    "\n",
    "% Put some labels \n",
    "hold on;\n",
    "% Labels and Legend\n",
    "xlabel('Exam 1 score')\n",
    "ylabel('Exam 2 score')\n",
    "\n",
    "% Specified in plot order\n",
    "legend('Admitted', 'Not admitted')\n",
    "hold off;\n",
    "\n",
    "% fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "function p = predict(theta, X)\n",
    "%PREDICT Predict whether the label is 0 or 1 using learned logistic \n",
    "%regression parameters theta\n",
    "%   p = PREDICT(theta, X) computes the predictions for X using a \n",
    "%   threshold at 0.5 (i.e., if sigmoid(theta'*x) >= 0.5, predict 1)\n",
    "\n",
    "m = size(X, 1); % Number of training examples\n",
    "\n",
    "% You need to return the following variables correctly\n",
    "p = zeros(m, 1);\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Complete the following code to make predictions using\n",
    "%               your learned logistic regression parameters. \n",
    "%               You should set p to a vector of 0's and 1's\n",
    "%\n",
    "\n",
    "data_i_l = 1:size(X, 1);\n",
    "\n",
    "classify = @(real) 1*(real>=0.5)+0*(real<0.5);\n",
    "\n",
    "klass = @(data_i_a) classify(sigmoid(X(data_i_a, :) * theta));\n",
    "\n",
    "p = arrayfun(\n",
    "    klass,\n",
    "    data_i_l\n",
    ").';\n",
    "\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a student with scores 45 and 85, we predict an admission probability of 0.776289\n",
      "Expected value: 0.775 +/- 0.002\n",
      "\n",
      "Train Accuracy: 89.000000\n",
      "Expected accuracy (approx): 89.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%% ============== Part 4: Predict and Accuracies ==============\n",
    "%  After learning the parameters, you'll like to use it to predict the outcomes\n",
    "%  on unseen data. In this part, you will use the logistic regression model\n",
    "%  to predict the probability that a student with score 45 on exam 1 and \n",
    "%  score 85 on exam 2 will be admitted.\n",
    "%\n",
    "%  Furthermore, you will compute the training and test set accuracies of \n",
    "%  our model.\n",
    "%\n",
    "%  Your task is to complete the code in predict.m\n",
    "\n",
    "%  Predict probability for a student with score 45 on exam 1 \n",
    "%  and score 85 on exam 2 \n",
    "\n",
    "prob = sigmoid([1 45 85] * theta);\n",
    "fprintf(['For a student with scores 45 and 85, we predict an admission ' ...\n",
    "         'probability of %f\\n'], prob);\n",
    "fprintf('Expected value: 0.775 +/- 0.002\\n\\n');\n",
    "\n",
    "% Compute accuracy on our training set\n",
    "p = predict(theta, X);\n",
    "\n",
    "fprintf('Train Accuracy: %f\\n', mean(double(p == y)) * 100);\n",
    "fprintf('Expected accuracy (approx): 89.0\\n');\n",
    "fprintf('\\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% Machine Learning Online Class - Exercise 2: Logistic Regression\n",
    "%\n",
    "%  Instructions\n",
    "%  ------------\n",
    "%\n",
    "%  This file contains code that helps you get started on the second part\n",
    "%  of the exercise which covers regularization with logistic regression.\n",
    "%\n",
    "%  You will need to complete the following functions in this exericse:\n",
    "%\n",
    "%     sigmoid.m\n",
    "%     costFunction.m\n",
    "%     predict.m\n",
    "%     costFunctionReg.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "clear ; close all; clc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "function out = mapFeature(X1, X2)\n",
    "% MAPFEATURE Feature mapping function to polynomial features\n",
    "%\n",
    "%   MAPFEATURE(X1, X2) maps the two input features\n",
    "%   to quadratic features used in the regularization exercise.\n",
    "%\n",
    "%   Returns a new feature array with more features, comprising of \n",
    "%   X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..\n",
    "%\n",
    "%   Inputs X1, X2 must be the same size\n",
    "%\n",
    "\n",
    "degree = 6;\n",
    "out = ones(size(X1(:,1)));\n",
    "for i = 1:degree\n",
    "    for j = 0:i\n",
    "        out(:, end+1) = (X1.^(i-j)).*(X2.^j);\n",
    "    end\n",
    "end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "function [J, grads] = costFunctionReg(theta, X, y, lambda)\n",
    "%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization\n",
    "%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using\n",
    "%   theta as the parameter for regularized logistic regression and the\n",
    "%   gradient of the cost w.r.t. to the parameters. \n",
    "\n",
    "% Initialize some useful values\n",
    "m = length(y); % number of training examples\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "J = 0;\n",
    "grad = zeros(size(theta));\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Compute the cost of a particular choice of theta.\n",
    "%               You should set J to the cost.\n",
    "%               Compute the partial derivatives and set grad to the partial\n",
    "%               derivatives of the cost w.r.t. each parameter in theta\n",
    "\n",
    "hypothesis = @(X) sigmoid(X * theta);\n",
    "\n",
    "predictors = X;\n",
    "\n",
    "response = y;\n",
    "\n",
    "i_l_data = 1:size([X y], 1);\n",
    "\n",
    "i_l_theta = 1:size(theta, 1);\n",
    "\n",
    "term_reg = @(i_a_theta) theta(i_a_theta)^2;\n",
    "\n",
    "sum_regularization = sum(\n",
    "    arrayfun(\n",
    "        term_reg,\n",
    "        i_l_theta\n",
    "    )\n",
    ");\n",
    "\n",
    "right = @(i_a_data) (1 - response(i_a_data))*log(1-hypothesis(predictors(i_a_data, :)));\n",
    "\n",
    "left = @(i_a_data) -1*response(i_a_data)*log(hypothesis(predictors(i_a_data, :)));\n",
    "\n",
    "loss_term = @(i_a_data) left(i_a_data) - right(i_a_data);\n",
    "\n",
    "sum_loss = sum(\n",
    "    arrayfun(\n",
    "        loss_term,\n",
    "        i_l_data\n",
    "    )\n",
    ");\n",
    "\n",
    "J = (1/m)*(sum_loss)+(lambda/(2*m))*sum_regularization;\n",
    "\n",
    "term_reg_grad = @(i_a_theta) (lambda/m)*theta(i_a_theta);\n",
    "\n",
    "term_grad = @(i_a_theta)...\n",
    "    @(i_a_data) (hypothesis(predictors(i_a_data, :)) - response(i_a_data)) * predictors(i_a_data, i_a_theta);\n",
    "\n",
    "sum_grad = @(i_a_theta) sum(...\n",
    "    arrayfun(...\n",
    "        term_grad(i_a_theta),...\n",
    "        i_l_data...\n",
    "    )...\n",
    ");\n",
    "\n",
    "grad = @(i_a_theta) ((1/m) * sum_grad(i_a_theta)) + term_reg_grad(i_a_theta);\n",
    "\n",
    "grads = arrayfun(\n",
    "    grad,\n",
    "    i_l_theta\n",
    ");\n",
    "\n",
    "grads(1) = grads(1) - (lambda/m)*theta(1);\n",
    "\n",
    "% =============================================================\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code_dir = /Users/elliottevers/Documents/git-repos.nosync/machine_learning/labs/3/machine-learning-ex2/ex2/\n",
      "Cost at initial theta (zeros): 0.693147\n",
      "Expected cost (approx): 0.693\n",
      "Gradient at initial theta (zeros) - first five values only:\n",
      " 0.008475 \n",
      " 0.018788 \n",
      " 0.000078 \n",
      " 0.050345 \n",
      " 0.011501 \n",
      "Expected gradients (approx) - first five values only:\n",
      " 0.0085\n",
      " 0.0188\n",
      " 0.0001\n",
      " 0.0503\n",
      " 0.0115\n"
     ]
    }
   ],
   "source": [
    "%% Load Data\n",
    "%  The first two columns contains the X values and the third column\n",
    "%  contains the label (y).\n",
    "code_dir = \"/Users/elliottevers/Documents/git-repos.nosync/machine_learning/labs/3/machine-learning-ex2/ex2/\"\n",
    "data = load(strcat(code_dir, 'ex2data2.txt'));\n",
    "X = data(:, [1, 2]); y = data(:, 3);\n",
    "\n",
    "plotData(X, y);\n",
    "\n",
    "% Put some labels\n",
    "hold on;\n",
    "% Labels and Legend\n",
    "xlabel('Microchip Test 1')\n",
    "ylabel('Microchip Test 2')\n",
    "\n",
    "% Specified in plot order\n",
    "legend('y = 1', 'y = 0')\n",
    "hold off;\n",
    "\n",
    "\n",
    "%% =========== Part 1: Regularized Logistic Regression ============\n",
    "%  In this part, you are given a dataset with data points that are not\n",
    "%  linearly separable. However, you would still like to use logistic\n",
    "%  regression to classify the data points.\n",
    "%\n",
    "%  To do so, you introduce more features to use -- in particular, you add\n",
    "%  polynomial features to our data matrix (similar to polynomial\n",
    "%  regression).\n",
    "%\n",
    "\n",
    "% Add Polynomial Features\n",
    "\n",
    "% Note that mapFeature also adds a column of ones for us, so the intercept\n",
    "% term is handled\n",
    "X = mapFeature(X(:,1), X(:,2));\n",
    "\n",
    "% Initialize fitting parameters\n",
    "initial_theta = zeros(size(X, 2), 1);\n",
    "\n",
    "% Set regularization parameter lambda to 1\n",
    "lambda = 1;\n",
    "\n",
    "% Compute and display initial cost and gradient for regularized logistic\n",
    "% regression\n",
    "[cost, grad] = costFunctionReg(initial_theta, X, y, lambda);\n",
    "\n",
    "fprintf('Cost at initial theta (zeros): %f\\n', cost);\n",
    "fprintf('Expected cost (approx): 0.693\\n');\n",
    "fprintf('Gradient at initial theta (zeros) - first five values only:\\n');\n",
    "fprintf(' %f \\n', grad(1:5));\n",
    "fprintf('Expected gradients (approx) - first five values only:\\n');\n",
    "fprintf(' 0.0085\\n 0.0188\\n 0.0001\\n 0.0503\\n 0.0115\\n');\n",
    "\n",
    "% fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cost at test theta (with lambda = 10): 3.206882\n",
      "Expected cost (approx): 3.16\n",
      "Gradient at test theta - first five values only:\n",
      " 0.346045 \n",
      " 0.161352 \n",
      " 0.194796 \n",
      " 0.226863 \n",
      " 0.092186 \n",
      "Expected gradients (approx) - first five values only:\n",
      " 0.3460\n",
      " 0.1614\n",
      " 0.1948\n",
      " 0.2269\n",
      " 0.0922\n"
     ]
    }
   ],
   "source": [
    "% Compute and display cost and gradient\n",
    "% with all-ones theta and lambda = 10\n",
    "test_theta = ones(size(X,2),1);\n",
    "[cost, grad] = costFunctionReg(test_theta, X, y, 10);\n",
    "\n",
    "fprintf('\\nCost at test theta (with lambda = 10): %f\\n', cost);\n",
    "fprintf('Expected cost (approx): 3.16\\n');\n",
    "fprintf('Gradient at test theta - first five values only:\\n');\n",
    "fprintf(' %f \\n', grad(1:5));\n",
    "fprintf('Expected gradients (approx) - first five values only:\\n');\n",
    "fprintf(' 0.3460\\n 0.1614\\n 0.1948\\n 0.2269\\n 0.0922\\n');\n",
    "\n",
    "% fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% ============= Part 2: Regularization and Accuracies =============\n",
    "%  Optional Exercise:\n",
    "%  In this part, you will get to try different values of lambda and\n",
    "%  see how regularization affects the decision coundart\n",
    "%\n",
    "%  Try the following values of lambda (0, 1, 10, 100).\n",
    "%\n",
    "%  How does the decision boundary change when you vary lambda? How does\n",
    "%  the training set accuracy vary?\n",
    "%\n",
    "\n",
    "% Initialize fitting parameters\n",
    "initial_theta = zeros(size(X, 2), 1);\n",
    "\n",
    "% Set regularization parameter lambda to 1 (you should vary this)\n",
    "lambda = 1;\n",
    "\n",
    "% Set Options\n",
    "options = optimset('GradObj', 'on', 'MaxIter', 400);\n",
    "\n",
    "% Optimize\n",
    "[theta, J, exit_flag] = ...\n",
    "\tfminunc(@(t)(costFunctionReg(t, X, y, lambda)), initial_theta, options);\n",
    "\n",
    "% Plot Boundary\n",
    "plotDecisionBoundary(theta, X, y);\n",
    "hold on;\n",
    "title(sprintf('lambda = %g', lambda))\n",
    "\n",
    "% Labels and Legend\n",
    "xlabel('Microchip Test 1')\n",
    "ylabel('Microchip Test 2')\n",
    "\n",
    "legend('y = 1', 'y = 0', 'Decision boundary')\n",
    "hold off;\n",
    "\n",
    "% Compute accuracy on our training set\n",
    "p = predict(theta, X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 83.898305\n",
      "Expected accuracy (with lambda = 1): 83.1 (approx)\n"
     ]
    }
   ],
   "source": [
    "fprintf('Train Accuracy: %f\\n', mean(double(p == y)) * 100);\n",
    "fprintf('Expected accuracy (with lambda = 1): 83.1 (approx)\\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "function plotDecisionBoundary(theta, X, y)\n",
    "%PLOTDECISIONBOUNDARY Plots the data points X and y into a new figure with\n",
    "%the decision boundary defined by theta\n",
    "%   PLOTDECISIONBOUNDARY(theta, X,y) plots the data points with + for the \n",
    "%   positive examples and o for the negative examples. X is assumed to be \n",
    "%   a either \n",
    "%   1) Mx3 matrix, where the first column is an all-ones column for the \n",
    "%      intercept.\n",
    "%   2) MxN, N>3 matrix, where the first column is all-ones\n",
    "\n",
    "% Plot Data\n",
    "plotData(X(:,2:3), y);\n",
    "hold on\n",
    "\n",
    "if size(X, 2) <= 3\n",
    "    % Only need 2 points to define a line, so choose two endpoints\n",
    "    plot_x = [min(X(:,2))-2,  max(X(:,2))+2];\n",
    "\n",
    "    % Calculate the decision boundary line\n",
    "    plot_y = (-1./theta(3)).*(theta(2).*plot_x + theta(1));\n",
    "\n",
    "    % Plot, and adjust axes for better viewing\n",
    "    plot(plot_x, plot_y)\n",
    "    \n",
    "    % Legend, specific for the exercise\n",
    "    legend('Admitted', 'Not admitted', 'Decision Boundary')\n",
    "    axis([30, 100, 30, 100])\n",
    "else\n",
    "    % Here is the grid range\n",
    "    u = linspace(-1, 1.5, 50);\n",
    "    v = linspace(-1, 1.5, 50);\n",
    "\n",
    "    z = zeros(length(u), length(v));\n",
    "    % Evaluate z = theta*x over the grid\n",
    "    for i = 1:length(u)\n",
    "        for j = 1:length(v)\n",
    "            z(i,j) = mapFeature(u(i), v(j))*theta;\n",
    "        end\n",
    "    end\n",
    "    z = z'; % important to transpose z before calling contour\n",
    "\n",
    "    % Plot z = 0\n",
    "    % Notice you need to specify the range [0, 0]\n",
    "    contour(u, v, z, [0, 0], 'LineWidth', 2)\n",
    "end\n",
    "hold off\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDecisionBoundary(theta, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "4.3.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
