{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% Initialization\n",
    "clear ; close all; clc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "function A = warmUpExercise()\n",
    "%WARMUPEXERCISE Example function in octave\n",
    "%   A = WARMUPEXERCISE() is an example function that returns the 5x5 identity matrix\n",
    "\n",
    "A = [];\n",
    "% ============= YOUR CODE HERE ==============\n",
    "% Instructions: Return the 5x5 identity matrix \n",
    "%               In octave, we return values by defining which variables\n",
    "%               represent the return values (at the top of the file)\n",
    "%               and then set them accordingly. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A = eye(5);\n",
    "% ===========================================\n",
    "\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running warmUpExercise ... \n",
      "5x5 Identity Matrix: \n",
      "ans =\n",
      "\n",
      "Diagonal Matrix\n",
      "\n",
      "   1   0   0   0   0\n",
      "   0   1   0   0   0\n",
      "   0   0   1   0   0\n",
      "   0   0   0   1   0\n",
      "   0   0   0   0   1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%% Machine Learning Online Class - Exercise 1: Linear Regression\n",
    "\n",
    "%  Instructions\n",
    "%  ------------\n",
    "%\n",
    "%  This file contains code that helps you get started on the\n",
    "%  linear exercise. You will need to complete the following functions\n",
    "%  in this exericse:\n",
    "%\n",
    "%     warmUpExercise.m\n",
    "%     plotData.m\n",
    "%     gradientDescent.m\n",
    "%     computeCost.m\n",
    "%     gradientDescentMulti.m\n",
    "%     computeCostMulti.m\n",
    "%     featureNormalize.m\n",
    "%     normalEqn.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "% x refers to the population size in 10,000s\n",
    "% y refers to the profit in $10,000s\n",
    "%\n",
    "\n",
    "%% ==================== Part 1: Basic Function ====================\n",
    "% Complete warmUpExercise.m\n",
    "fprintf('Running warmUpExercise ... \\n');\n",
    "fprintf('5x5 Identity Matrix: \\n');\n",
    "warmUpExercise()\n",
    "\n",
    "% fprintf('Program paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = \"/Users/elliottevers/Documents/git-repos.nosync/machine_learning/labs/2/machine-learning-ex1/ex1/\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%plot gnuplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "function plotData(x, y)\n",
    "%PLOTDATA Plots the data points x and y into a new figure \n",
    "%   PLOTDATA(x,y) plots the data points and gives the figure axes labels of\n",
    "%   population and profit.\n",
    "\n",
    "% figure; % open a new figure window\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Plot the training data into a figure using the \n",
    "%               \"figure\" and \"plot\" commands. Set the axes labels using\n",
    "%               the \"xlabel\" and \"ylabel\" commands. Assume the \n",
    "%               population and revenue data have been passed in\n",
    "%               as the x and y arguments of this function.\n",
    "%\n",
    "% Hint: You can use the 'rx' option with plot to have the markers\n",
    "%       appear as red crosses. Furthermore, you can make the\n",
    "%       markers larger by using plot(..., 'rx', 'MarkerSize', 10);\n",
    "\n",
    "plot(x, y, 'rx', 'MarkerSize', 10); % Plot the data\n",
    "ylabel('Profit in $10,000s'); % Set the y−axis label\n",
    "xlabel('Population of City in 10,000s'); % Set the x−axis label\n",
    "% ============================================================\n",
    "figure;\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting Data ...\n"
     ]
    }
   ],
   "source": [
    "%% ======================= Part 2: Plotting =======================\n",
    "fprintf('Plotting Data ...\\n')\n",
    "data = load(strcat(doc_dir, 'ex1data1.txt'));\n",
    "X = data(:, 1); y = data(:, 2);\n",
    "m = length(y); % number of training examples\n",
    "\n",
    "% Plot Data\n",
    "% Note: You have to complete the code in plotData.m\n",
    "% plotData(X, y);\n",
    "\n",
    "% fprintf('Program paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def hypothesis(\n",
    "    Real x,\n",
    "    Array[Real] coefficients\n",
    ") -> Real:\n",
    "    sequence = [\n",
    "        coefficient * (x ^ index)\n",
    "        for index, coefficient\n",
    "        in coefficients\n",
    "    ]\n",
    "    return sum(sequence)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Cost Function\n",
    "def J(\n",
    "    Data data,\n",
    "    Callable[Real, Real] hypothesis\n",
    ") -> Real:\n",
    "    quotient = 1/ 2*length(data)\n",
    "\n",
    "    sum = sum([\n",
    "        (hypothesis(predictor) - observed) ^ 2\n",
    "        for predictor, observed\n",
    "        in data\n",
    "    ])\n",
    "\n",
    "    return product(\n",
    "        quotient,\n",
    "        sum\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "function J = computeCost(X, y, theta)\n",
    "%COMPUTECOST Compute cost for linear regression\n",
    "%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the\n",
    "%   parameter for linear regression to fit the data points in X and y\n",
    "\n",
    "% Initialize some useful values\n",
    "m = length(y); % number of training examples\n",
    "\n",
    "% You need to return the following variables correctly \n",
    "J = 0;\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: Compute the cost of a particular choice of theta\n",
    "%               You should set J to the cost.\n",
    "zipped = [X, y];\n",
    "\n",
    "hypothesis = @(x, theta) theta(1) + ((x ^ 1) * theta(2));\n",
    "\n",
    "quotient = 1/(2*m);\n",
    "\n",
    "cost_sum = 0;\n",
    "\n",
    "for i = 1:m\n",
    "    predictor = zipped(i, 2);\n",
    "    observed = zipped(i, 3);\n",
    "    cost_sum = cost_sum + (hypothesis(predictor, theta) - observed) ^ 2;\n",
    "end\n",
    "\n",
    "J = quotient * cost_sum;\n",
    "\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the cost function ...\n",
      "With theta = [0 ; 0]\n",
      "Cost computed = 32.072734\n",
      "Expected cost value (approx) 32.07\n",
      "\n",
      "With theta = [-1 ; 2]\n",
      "Cost computed = 54.242455\n",
      "Expected cost value (approx) 54.24\n"
     ]
    }
   ],
   "source": [
    "%% =================== Part 3: Cost and Gradient descent ===================\n",
    "\n",
    "X = [ones(m, 1), data(:,1)]; % Add a column of ones to x\n",
    "% essentially zips the two arrays\n",
    "theta = zeros(2, 1); % initialize fitting parameters\n",
    "% transpose of row vector\n",
    "\n",
    "% Some gradient descent settings\n",
    "iterations = 1500;\n",
    "alpha = 0.01;\n",
    "\n",
    "fprintf('\\nTesting the cost function ...\\n')\n",
    "% compute and display initial cost\n",
    "J = computeCost(X, y, theta);\n",
    "fprintf('With theta = [0 ; 0]\\nCost computed = %f\\n', J);\n",
    "fprintf('Expected cost value (approx) 32.07\\n');\n",
    "\n",
    "% further testing of the cost function\n",
    "J = computeCost(X, y, [-1 ; 2]);\n",
    "fprintf('\\nWith theta = [-1 ; 2]\\nCost computed = %f\\n', J);\n",
    "fprintf('Expected cost value (approx) 54.24\\n');\n",
    "\n",
    "% fprintf('Program paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def batch_gradient_descent(\n",
    "    Array[Real] coefficients,\n",
    "    Data data,\n",
    "    Callable[Real, Real] hypothesis,\n",
    "    Integer num_batches,\n",
    "    Real learning_rate\n",
    ") -> Array[Real]:\n",
    "\n",
    "    m = length(data)\n",
    "\n",
    "    quotient = 1/m\n",
    "\n",
    "    for batch_num in num_batches:\n",
    "        for coefficient in coefficients:\n",
    "            sum = sum([\n",
    "                (hypothesis(predictor) - response) * predictor\n",
    "                for predictor, response\n",
    "                in data\n",
    "            ])\n",
    "        coefficient = coefficient - (learning_rate * quotient * sum)\n",
    "\n",
    "    return coefficients\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans =  4\n"
     ]
    }
   ],
   "source": [
    "% f = @(c,x)( @(x)(x^3+c*x^2+2*x+3) );\n",
    "\n",
    "% for c=1:10\n",
    "%     g=f(c) % g is @(x)(x^3+c*x^2+2*x+3) for that c\n",
    "% end\n",
    "lam1 = @(arg1, arg2) @(arg2)(arg1 + arg2);\n",
    "lam1(1)(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "% function result = term(i_data)\n",
    "\n",
    "%     predictors = data(i_data, i_predictor1:i_predictor2);\n",
    "    \n",
    "%     result = (hypothesis(predictors) - response) * data(i_data, i_theta);\n",
    "    \n",
    "% end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "% function result = update(i_theta)\n",
    "\n",
    "%     i_data = 1:size(data, 1);\n",
    "\n",
    "%     terms = arrayfun(term, i_data);\n",
    "\n",
    "%     summ = sum(terms);\n",
    "\n",
    "%     result = theta(i_theta) - alpha * (1/m) * summ;\n",
    "\n",
    "% end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "% function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)\n",
    "% %GRADIENTDESCENT Performs gradient descent to learn theta\n",
    "% %   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by \n",
    "% %   taking num_iters gradient steps with learning rate alpha\n",
    "\n",
    "% % Initialize some useful values\n",
    "% m = length(y); % number of training examples\n",
    "% J_history = zeros(num_iters, 1);\n",
    "\n",
    "% data = [X, y];\n",
    "\n",
    "% i_predictor_1 = 1;\n",
    "% i_predictor_2 = 2;\n",
    "\n",
    "% i_degree_0 = 1;\n",
    "% i_degree_1 = 2;\n",
    "\n",
    "% for iter = 1:num_iters\n",
    "\n",
    "%     % ====================== YOUR CODE HERE ======================\n",
    "%     % Instructions: Perform a single gradient step on the parameter vector\n",
    "%     %               theta. \n",
    "%     %\n",
    "%     % Hint: While debugging, it can be useful to print out the values\n",
    "%     %       of the cost function (computeCost) and gradient here.\n",
    "%     %\n",
    "%     hypothesis = @(x) theta(i_degree_0) + (x^1*i_degree_1);\n",
    "    \n",
    "%     i_theta = 1:size(theta, 1);\n",
    "\n",
    "%     function result_update = update(i_theta)\n",
    "        \n",
    "%         i_data = 1:size(data, 1);\n",
    "        \n",
    "%         function result_term = term(i_data)\n",
    "            \n",
    "%             predictors = data(i_data, i_predictor1:i_predictor2);\n",
    "\n",
    "%             result_term = (hypothesis(predictors) - response) * data(i_data, i_theta);\n",
    "\n",
    "%         end\n",
    "        \n",
    "%         terms = arrayfun(term, i_data);\n",
    "\n",
    "%         summ = sum(terms);\n",
    "\n",
    "%         result_update = theta(i_theta) - alpha * (1/m) * summ;\n",
    "\n",
    "%     end\n",
    "    \n",
    "%     theta = arrayfun(update, i_theta);\n",
    "    \n",
    "%     % Save the cost J in every iteration    \n",
    "%     J_history(iter) = computeCost(X, y, theta);\n",
    "\n",
    "% end\n",
    "\n",
    "% end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)\n",
    "%GRADIENTDESCENT Performs gradient descent to learn theta\n",
    "%   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by \n",
    "%   taking num_iters gradient steps with learning rate alpha\n",
    "\n",
    "% Initialize some useful values\n",
    "m = length(y); % number of training examples\n",
    "J_history = zeros(num_iters, 1);\n",
    "\n",
    "data = [X, y];\n",
    "\n",
    "i_predictor_1 = 1;\n",
    "\n",
    "i_predictor_2 = 2;\n",
    "\n",
    "i_response = 3;\n",
    "\n",
    "# column index, column indices, row index, row indices, index arg\n",
    "# magic numbers at top\n",
    "# pass indices to anonymous functions\n",
    "\n",
    "for iter = 1:num_iters\n",
    "\n",
    "    % ====================== YOUR CODE HERE ======================\n",
    "    % Instructions: Perform a single gradient step on the parameter vector\n",
    "    %               theta. \n",
    "    %\n",
    "    % Hint: While debugging, it can be useful to print out the values\n",
    "    %       of the cost function (computeCost) and gradient here.\n",
    "    %\n",
    "\n",
    "    hypothesis = @(predictors) theta(1) + theta(2)*predictors(2);\n",
    "    \n",
    "    is_theta = 1:size(theta, 1);\n",
    "    \n",
    "    is_data = 1:size(data, 1);\n",
    "    \n",
    "    predictors = data(:, i_predictor_1:i_predictor_2);\n",
    "    \n",
    "    response = data(:, i_response);\n",
    "\n",
    "    term = @(i_theta, i_data)...\n",
    "        @(i_data) (hypothesis(predictors(i_data, :)) - response(i_data)) * predictors(i_data, i_theta);\n",
    "    \n",
    "    summ = @(i_theta)...\n",
    "        sum(...\n",
    "            arrayfun(...\n",
    "                term(i_theta),...\n",
    "                is_data...\n",
    "            )...\n",
    "        );...\n",
    "    \n",
    "    update = @(i_theta)...\n",
    "        theta(i_theta) - alpha * (1/m) * summ(i_theta);\n",
    "    \n",
    "    theta = arrayfun(\n",
    "        update,\n",
    "        is_theta\n",
    "    ).';\n",
    "    \n",
    "    % Save the cost J in every iteration    \n",
    "    J_history(iter) = computeCost(X, y, theta);\n",
    "\n",
    "end\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "% dbstop in gradientDescent 54\n",
    "gradientDescent(X, y, theta, alpha, iterations);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Gradient Descent ...\n",
      "Theta found by gradient descent:\n",
      "-3.630291\n",
      "1.166362\n",
      "Expected theta values (approx)\n",
      " -3.6303\n",
      "  1.1664\n",
      "\n",
      "warning: legend: ignoring extra labels\n",
      "warning: called from\n",
      "    legend at line 468 column 13\n",
      "For population = 35,000, we predict a profit of 4519.767868\n",
      "For population = 70,000, we predict a profit of 45342.450129\n"
     ]
    }
   ],
   "source": [
    "fprintf('\\nRunning Gradient Descent ...\\n')\n",
    "% run gradient descent\n",
    "theta = gradientDescent(X, y, theta, alpha, iterations);\n",
    "\n",
    "% print theta to screen\n",
    "fprintf('Theta found by gradient descent:\\n');\n",
    "fprintf('%f\\n', theta);\n",
    "fprintf('Expected theta values (approx)\\n');\n",
    "fprintf(' -3.6303\\n  1.1664\\n\\n');\n",
    "\n",
    "% Plot the linear fit\n",
    "hold on; % keep previous plot visible\n",
    "plot(X(:,2), X*theta, '-')\n",
    "legend('Training data', 'Linear regression')\n",
    "hold off % don't overlay any more plots on this figure\n",
    "\n",
    "% Predict values for population sizes of 35,000 and 70,000\n",
    "predict1 = [1, 3.5] *theta;\n",
    "fprintf('For population = 35,000, we predict a profit of %f\\n',...\n",
    "    predict1*10000);\n",
    "predict2 = [1, 7] * theta;\n",
    "fprintf('For population = 70,000, we predict a profit of %f\\n',...\n",
    "    predict2*10000);\n",
    "\n",
    "% fprintf('Program paused. Press enter to continue.\\n');\n",
    "% pause;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing J(theta_0, theta_1) ...\n"
     ]
    }
   ],
   "source": [
    "%% ============= Part 4: Visualizing J(theta_0, theta_1) =============\n",
    "fprintf('Visualizing J(theta_0, theta_1) ...\\n')\n",
    "\n",
    "% Grid over which we will calculate J\n",
    "theta0_vals = linspace(-10, 10, 100);\n",
    "theta1_vals = linspace(-1, 4, 100);\n",
    "\n",
    "% initialize J_vals to a matrix of 0's\n",
    "J_vals = zeros(length(theta0_vals), length(theta1_vals));\n",
    "\n",
    "% Fill out J_vals\n",
    "for i = 1:length(theta0_vals)\n",
    "    for j = 1:length(theta1_vals)\n",
    "\t  t = [theta0_vals(i); theta1_vals(j)];\n",
    "\t  J_vals(i,j) = computeCost(X, y, t);\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "% Because of the way meshgrids work in the surf command, we need to\n",
    "% transpose J_vals before calling surf, or else the axes will be flipped\n",
    "J_vals = J_vals';\n",
    "% Surface plot\n",
    "figure;\n",
    "surf(theta0_vals, theta1_vals, J_vals)\n",
    "xlabel('\\theta_0'); ylabel('\\theta_1');\n",
    "\n",
    "% Contour plot\n",
    "figure;\n",
    "% Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\n",
    "contour(theta0_vals, theta1_vals, J_vals, logspace(-2, 3, 20))\n",
    "xlabel('\\theta_0'); ylabel('\\theta_1');\n",
    "hold on;\n",
    "plot(theta(1), theta(2), 'rx', 'MarkerSize', 10, 'LineWidth', 2);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "4.3.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
